{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os\n",
    "import json\n",
    "from weaviate.util import generate_uuid5\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Specify the path to your PDF file\n",
    "# pdf_path = '../data/test/CSC411f18_midterm1.pdf'\n",
    "\n",
    "# elements = partition_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed elements from /Users/ceciliaacosta/Project_IFT/IFT6759DesktopAgent/data/test/CSC311f19_final.pdf\n",
      "Processed elements from /Users/ceciliaacosta/Project_IFT/IFT6759DesktopAgent/data/test/CSC411f18_midterm2.pdf\n",
      "Processed elements from /Users/ceciliaacosta/Project_IFT/IFT6759DesktopAgent/data/test/CSC411f18_midterm1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the PDF files\n",
    "directory_path = '/Users/ceciliaacosta/Project_IFT/IFT6759DesktopAgent/data/test'\n",
    "\n",
    "# Dictionary to hold file names and their elements\n",
    "files_elements = {}\n",
    "\n",
    "# Find all PDF files in the specified directory\n",
    "pdf_files = glob.glob(os.path.join(directory_path, '*.pdf'))\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    # Extract elements from the PDF\n",
    "    elements = partition_pdf(pdf_file)\n",
    "\n",
    "    \n",
    "    # Convert each element to a string (assuming each element has a .text attribute or similar)\n",
    "    # Adjust this line if the elements are structured differently\n",
    "    elements_text = [str(elem.text) if hasattr(elem, 'text') else str(elem) for elem in elements]\n",
    "    \n",
    "    # Extract the file name without extension to use as a key\n",
    "    file_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "    \n",
    "    # Store the elements in the dictionary\n",
    "    files_elements[file_name] = elements_text\n",
    "\n",
    "    print(f\"Processed elements from {pdf_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_questions_refined(elements):\n",
    "    text = \"\\n\".join(str(elem) for elem in elements)\n",
    "    \n",
    "    # Patterns for main questions and subquestions\n",
    "    main_question_pattern = re.compile(r'(\\d+)\\.\\s*(.*?)\\s*(?=\\n\\d+\\.|$)', re.DOTALL)\n",
    "    subquestion_pattern = re.compile(r'\\n\\s*\\(([a-z])\\)\\s*(.*?)(?=\\n\\s*\\([a-z]\\)\\s*|\\n\\d+\\.|$)', re.DOTALL)\n",
    "    \n",
    "    questions = {}\n",
    "    \n",
    "    # Match and store main questions\n",
    "    for main_match in main_question_pattern.finditer(text):\n",
    "        main_question_number = main_match.group(1)\n",
    "        main_question_text = main_match.group(2).strip()\n",
    "        questions[main_question_number] = {\n",
    "            \"text\": main_question_text.split(\"\\n\", 1)[0],\n",
    "            \"subquestions\": []\n",
    "        }\n",
    "\n",
    "    # Match and store subquestions\n",
    "    for sq_match in subquestion_pattern.finditer(text):\n",
    "        subquestion_letter = sq_match.group(1).strip()\n",
    "        subquestion_text = sq_match.group(2).strip()\n",
    "\n",
    "        # Determine the appropriate main question for each subquestion\n",
    "        previous_main_question_number = None\n",
    "        for mq_number in questions:\n",
    "            if text.find(\"\\n\" + mq_number + \".\") < sq_match.start():\n",
    "                previous_main_question_number = mq_number\n",
    "\n",
    "        # Add the subquestion to the identified main question\n",
    "        if previous_main_question_number:\n",
    "            questions[previous_main_question_number][\"subquestions\"].append((subquestion_letter, subquestion_text))\n",
    "   \n",
    "    return questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_parse_questions_to_files_elements(files_elements):\n",
    "    # Structure to hold the parsed questions for each file\n",
    "    parsed_data = {}\n",
    "\n",
    "    # Iterate through each file's elements in the dictionary\n",
    "    for file_name, elements in files_elements.items():\n",
    "        # Apply the parse_questions function\n",
    "        parsed_questions = parse_questions_refined(elements)\n",
    "        \n",
    "        # Store the parsed questions for this file\n",
    "        parsed_data[file_name] = parsed_questions\n",
    "    \n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: CSC311f19_final\n",
      "Question 8: 5(cid:48)(cid:48) × 11(cid:48)(cid:48) or A4 aid sheets.\n",
      "Question 1: Is EM algorithm a supervised or an unsupervised learning method? Explain your answer.\n",
      "Question 2: How does EM algorithm and k-means compare? Write 3 similarities and 3 diﬀerences.\n",
      "Question 3: Explain why we call these steps expectation and maximization steps. What is it that we take expectation of and what is it that we maximize?\n",
      "Question 4: Principal Component Analysis. Recall that the optimal PCA subspace can be deter- mined from the eigendecomposition of the empirical covariance matrix ˆΣ. Also recall that the eigendecomposition can be expressed in terms of the following spectral decomposition of ˆΣ: ˆΣ = QΛQ(cid:62),\n",
      "Question 5: Support Vector Machines. Support vector machines learn a decision boundary leading to the largest margin from both classes. You are training SVM on a tiny dataset with 4 points shown below. This dataset consists of two examples with class label -1 (denoted with plus), and two examples with class label +1 (denoted with triangles).\n",
      "Question 6: Probabilistic Models . The Laplace distribution, parameterized by µ and β, is deﬁned as follows:\n",
      "Question 7: EM Algorithm.\n",
      "\n",
      "\n",
      "File: CSC411f18_midterm2\n",
      "Question 1: [1pt] Give one reason why an algorithm implemented in terms of matrix and vector operations can be faster than the same algorithm implemented in terms of for-loops.\n",
      "Question 2: [2pts] Brieﬂy explain two advantages of decision trees over K-nearest-neighbors.\n",
      "Question 3: [1pt] TRUE or FALSE: AdaBoost will eventually choose a weak classiﬁer that achieves a weighted error rate of 0. Brieﬂy justify your answer.\n",
      "Question 4: [1pt] In class, we considered using the squared Euclidean norm of the weights, (cid:107)w(cid:107)2, as a regularizer. Suppose we instead used the Euclidean norm (cid:107)w(cid:107) as a regularizer (i.e. without squaring it). Brieﬂy explain one way in which this would lead to diﬀerent behavior.\n",
      "Question 5: [2pts] Recall that hyperparameters are often tuned using a validation set.\n",
      " ----sub a: [1pt] Give an example of a hyperparameter which it is OK to tune on the training\n",
      "set (rather than a validation set). Brieﬂy explain your answer.\n",
      " ----sub b: [1pt] Give an example of a hyperparameter which should be tuned on a validation\n",
      "set, rather than the training set. Brieﬂy explain your answer.\n",
      "4\n",
      "CSC411/2515 Fall 2018\n",
      "Midterm Test, Version A\n",
      "Question 6: [2pts] In this question, you will write NumPy code to implement a 1-nearest-neighbour classiﬁer. Assume you are given an N × D data matrix X, where each row corresponds to one of the input vectors, and an integer array y with the corresponding labels. (You may assume the labels are integers from 1 to K.) You are given a query vector x_query. Your job is to return the predicted class (as an integer). Do not use a for-loop. If you don’t remember the API for a NumPy operation, then for partial credit, explain what you are trying to do.\n",
      "Question 7: [2pts] Consider the classiﬁcation problem with the following dataset:\n",
      " ----sub a: [1pt] Give the set of linear inequalities the weights and bias must satisfy.\n",
      " ----sub b: [1pt] Give a setting of the weights and bias that correctly classiﬁes all the training examples. You don’t need to show your work, but it might help you get partial credit.\n",
      "6\n",
      "CSC411/2515 Fall 2018\n",
      "Midterm Test, Version A\n",
      "Question 8: [2pts] Recall that in bagging, we compute an average of the predictions yavg = 1 m (cid:80)m\n",
      "Question 9: [2pts] Consider a regression problem where the input is a scalar x. Suppose we know that the dataset is generated by the following process. First, the target t is chosen from {0, 1} with equal probability. If t = 0, then x is sampled from a uniform distribution over the interval [1, 2]. If t = 1, then x is sampled from a uniform distribution over the interval [0, 2]. Give a function f (x), deﬁned for x ∈ [0, 2], such that y∗ = f (x) is the Bayes optimal predictor for t given x. (Note that even though t is binary valued, this is a regression problem, with squared error loss.)\n",
      "\n",
      "\n",
      "File: CSC411f18_midterm1\n",
      "Question 1: As discussed in lecture, when applying K-nearest-neighbors, it is common to normalize each input dimension to unit variance.\n",
      " ----sub a: [1pt] Why might it be advantageous to do this?\n",
      " ----sub b: [1pt] When might this normalization step not be a good idea? (Hint: You may want to consider the task of classifying images of handwritten digits, where the digit is centered within the image.)\n",
      "Question 2: [1pt] In random forests, what is the motivation for randomizing the set of attributes considered for each split?\n",
      "Question 3: [1pt] Suppose you want to evaluate the test error rate of a 1-nearest-neighbors classiﬁer. Assume you implement the algorithm the na¨ıve way, i.e. by explicitly computing all the distances and taking the min, rather than by using a fancy data structure. What is the running time of evaluating the test error? Give your answer in big-O notation, in terms of the number of training examples Ntrain, the number of test examples Ntest, and the input dimension D. Brieﬂy explain your answer.\n",
      "Question 4: (a) [1pt] Give one advantage of K-nearest-neighbors over linear regression.\n",
      " ----sub a: [1pt] Give one advantage of K-nearest-neighbors over linear regression.\n",
      " ----sub b: [1pt] Give one advantage of linear regression over K-nearest-neighbors.\n",
      "4\n",
      "CSC411/2515 Fall 2018\n",
      "Midterm Test, Version B\n",
      "Question 5: [1pt] Suppose linear regression (with squared error loss) is used as a classiﬁcation algorithm. TRUE or FALSE: if it correctly classiﬁes every training example, then its cost is zero. (By “cost”, we mean the function minimized during training.) Brieﬂy justify your answer.\n",
      "Question 6: [2pts] Let Z be a random variable and t be a real number. Show that\n",
      "Question 7: [2pts] Suppose binary-valued random variables X and Y have the following joint distribution:\n",
      "Question 8: [2pts] Recall that combining the logistic activation function with squared error loss suﬀers from saturation, whereby the gradient signal is very small when the prediction for a training example is very wrong. Logistic regression (i.e. logistic activation function with cross-entropy loss) doesn’t have this problem. Recall that the logistic function is deﬁned as σ(z) = 1/(1 + e−z). Now suppose we modify the activation function to squash the prediction y to be in the interval [0.1, 0.9], and then apply cross-entropy loss. I.e., z = w(cid:62)x + b y = 0.8σ(z) + 0.1\n",
      "Question 9: [2pts] Recall that the soft-margin SVM can be viewed as minimizing the hinge loss with an L2 regularization term. I.e.,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_name, elements in files_elements.items():\n",
    "    print(f\"File: {file_name}\")\n",
    "\n",
    "    # Apply the parse_questions function to the elements of the current file\n",
    "    parsed_questions = parse_questions_refined(elements)\n",
    "\n",
    "    # Iterate through the parsed questions and print them along with their subquestions\n",
    "    for question_num, question_info in parsed_questions.items():\n",
    "        print(f\"Question {question_num}: {question_info['text']}\")\n",
    "        for subq in question_info[\"subquestions\"]:\n",
    "            \n",
    "            print(f\" ----sub {subq[0]}: {subq[1]}\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ceciliaacosta/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(\n",
    "        additional_env_vars={\"X-HuggingFace-Api-Key\": \"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\"}\n",
    "    )\n",
    ")\n",
    "assert client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard test_PsheZ8RUUm9z in 3.291323ms\",\"time\":\"2024-03-09T09:59:15-05:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-09T09:59:15-05:00\",\"took\":116026}\n"
     ]
    }
   ],
   "source": [
    "client.schema.delete_all()\n",
    "# Create a new class with a vectorizer\n",
    "schema = {\n",
    "    \"class\": \"Test\",    \n",
    "    \"vectorizer\": \"text2vec-huggingface\",\n",
    "    \"moduleConfig\": {\n",
    "    \"text2vec-huggingface\": {\n",
    "      \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Can be any public or private Hugging Face model.\n",
    "      \"options\": {\n",
    "        \"waitForModel\": True,  # Try this if you get a \"model not ready\" error\n",
    "      }\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "client.schema.create_class(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing question: 1\n",
      "importing question: 2\n",
      "importing question: 3\n",
      "importing question: 4\n",
      "importing question: 5\n",
      "importing question: 6\n",
      "importing question: 7\n",
      "importing question: 8\n",
      "importing question: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client.batch.configure(batch_size=100)  # Configure batch\n",
    "with client.batch as batch:  # Initialize a batch process\n",
    "    for file_name, elements in files_elements.items():\n",
    "\n",
    "        parsed_questions = parse_questions_refined(elements)\n",
    "        \n",
    "        for question, details in parsed_questions.items():  # Batch import data\n",
    "            print(f\"importing question: {question}\")\n",
    "            for subq in details[\"subquestions\"]:\n",
    "                properties = {\n",
    "                    \"mainQuestion\": question,\n",
    "                    \"subQuestion\": subq[1]\n",
    "                \n",
    "                }\n",
    "                batch.add_data_object(\n",
    "                    data_object=properties,\n",
    "                    class_name=\"Test\"\n",
    "                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift-6758",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
