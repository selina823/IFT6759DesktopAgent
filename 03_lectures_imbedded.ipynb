{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(\n",
    "        additional_env_vars={\"X-HuggingFace-Api-Key\": \"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\"}\n",
    "    )\n",
    ")\n",
    "#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\n",
    "assert client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard test_JWRqUX6eGimL in 4.881169ms\",\"time\":\"2024-03-30T15:41:55-04:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-03-30T15:41:55-04:00\",\"took\":103540}\n"
     ]
    }
   ],
   "source": [
    "client.schema.delete_all()\n",
    "# Create a new class with a vectorizer\n",
    "schema = {\n",
    "    \"class\": \"Test\",    \n",
    "    \"vectorizer\": \"text2vec-huggingface\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"content\",  #What we want to vectorize\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"Content of PDF\",\n",
    "            \"moduleConfig\": {\n",
    "                \"text2vec-huggingface\": {\"skip\": False, \"vectorizePropertyName\": False}\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"filename\",\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"PDF filename\"\n",
    "        },\n",
    "    ],\n",
    "    \"moduleConfig\": {\n",
    "    \"text2vec-huggingface\": {\n",
    "      \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Can be any public or private Hugging Face model.\n",
    "      \"options\": {\n",
    "        \"waitForModel\": True,  # Try this if you get a \"model not ready\" error\n",
    "      }\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "client.schema.create_class(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing question: 1\n",
      "importing question: 2\n",
      "importing question: 3\n",
      "importing question: 4\n",
      "importing question: 5\n",
      "importing question: 6\n",
      "importing question: 7\n",
      "importing question: 8\n",
      "importing question: 9\n",
      "importing question: 10\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "resp = requests.get('https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json')\n",
    "data = json.loads(resp.text)  # Load data\n",
    "\n",
    "client.batch.configure(batch_size=100)  # Configure batch\n",
    "with client.batch as batch:  # Initialize a batch process\n",
    "    for i, d in enumerate(data):  # Batch import data\n",
    "        print(f\"importing question: {i+1}\")\n",
    "        properties = {\n",
    "            \"answer\": d[\"Answer\"],\n",
    "            \"question\": d[\"Question\"],\n",
    "            \"category\": d[\"Category\"],\n",
    "        }\n",
    "        batch.add_data_object(\n",
    "            data_object=properties,\n",
    "            class_name=\"Question\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     client\u001b[38;5;241m.\u001b[39mquery\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mwith_limit(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mdo()\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(response, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"Question\", [\"question\", \"answer\", \"category\"])\n",
    "\n",
    "    .with_limit(10)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import DataSourceMetadata\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from weaviate.util import generate_uuid5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(elements, chunk_under_n_chars=500, chunk_new_after_n_chars=1500):\n",
    "\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        multipage_sections=False, # If True, the title of the first page is used for all pages\n",
    "        combine_text_under_n_chars=chunk_under_n_chars,\n",
    "        new_after_n_chars=chunk_new_after_n_chars\n",
    " \n",
    "    )\n",
    "\n",
    "    for i in range(len(chunks)):\n",
    "        chunks[i] = {\"text\": chunks[i].text, \"filename\": chunks[i].metadata.filename}\n",
    "\n",
    "    chunk_texts = [x['text'] for x in chunks]\n",
    "    return chunks\n",
    "\n",
    "#@sleep_and_retry\n",
    "#@limits(calls=RATE_LIMIT, period=1)\n",
    "# def add_data_to_weaviate(files, client, chunk_under_n_chars=500, chunk_new_after_n_chars=1500):\n",
    "#     for filename in files:\n",
    "#         try:\n",
    "#             elements = partition_pdf(filename=filename)\n",
    "#             chunks = get_chunks(elements, chunk_under_n_chars, chunk_new_after_n_chars)\n",
    "#         except IndexError as e:\n",
    "#             print(e)\n",
    "#             continue\n",
    "\n",
    "#         print(f\"Uploading {len(chunks)} chunks for {str(filename)}.\")\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             try:\n",
    "#                 client.data_object.create(class_name=\"Test\", data_object={\"content\": chunk['text'], \"filename\": filename})\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 print(f\"Failed to upload chunk {i} for {str(filename)}.\")\n",
    "\n",
    "#         with client.batch as batch:\n",
    "#             for data_object in chunks:\n",
    "#                 batch.add_data_object(data_object={\"content\": chunk['text'], \"filename\": filename}, class_name=\"Test\", uuid=generate_uuid5(data_object))\n",
    "\n",
    "        \n",
    "#     client.batch.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 103 chunks for data/coursematerial/test_data/lec01.pdf.\n",
      "{'content': 'STA 314: Statistical Methods for Machine Learning I Lecture 1 - Introduction and Nearest Neighbours\\n\\nChris J. Maddison1', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'University of Toronto\\n\\n1Slides adapted from CSC 311.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n1 / 65\\n\\nAre you happy with fully online tutorials?\\n\\nI sent out a survey to measure preferences for in-person tutorials. Please take a moment to ﬁll it out:', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'https://forms.office.com/r/mzZ06k9Dfa\\n\\nYou need to sign into your UofT account to ﬁll it out.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n2 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '5XptJ09dB)Register by 6:00 p.m. ET on Sep 20.\\n\\nvoiceJoin phase 2 of a DoSS pilot: The UndergraduateConsultative CommitteeStudentTime commitment: 2–3 hours of meetings, 1–5 hours of emails & other engagement.Leadership • Empathy • Community • TransparencyRead what past reps had tosay about this program inthe', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Register here\\n\\nDepartment News.Student representatives work with their instructor to solicitand respond to student feedback throughout the semester.All participating instructors and reps meet twice a semesterfor a Consultative Committee meeting. Meetings:Thurs, Oct 7, at 1:00 p.m.&Thurs, Nov 18 at 1:00 p.m.How are representatives selected?Interested students', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'register\\n\\nat\\n\\n(forms.office.com/r/6\\n\\nforms.office.com/r/65XptJ09dB. The reps will be chosen by lottery from those who register theirinterest by 6:00 p.m. ET on Monday, Sep 20. Students will beinformed of the outcome in the week starting Sep 20.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n3 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'This course\\n\\nThis course is a broad introduction to machine learning. We cover two major types: Supervised learning\\n\\nnearest neighbours, decision trees, ensembles, linear regression, logistic regression, SVMs Unsupervised learning', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'PCA, K-means, probabilistic models\\n\\nWe cover a variety of important methods:\\n\\nmaximum likelihood, optimization with gradient descent, Bayesian inference\\n\\nCoursework is aimed at advanced undergrads. We will use multivariate calculus, probability, and linear algebra.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n4 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Do I have the appropriate background?\\n\\nLinear algebra: vector/matrix manipulations, properties.\\n\\nCalculus: partial derivatives/gradient.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Probability: common distributions; Bayes Rule.\\n\\nStatistics: expectation, variance, covariance, median; maximum likelihood.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n5 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Do I have the appropriate background?\\n\\nWe are using the Python programming language in this course. How much do you need to know?', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'The emphasis will be on the use of the numerical computing package NumPy (tutorial 2) and on the implementation of the key subroutines of ML methods. You will not need to write an entire Python package. We will provide you with very complete starter code. You will need to conﬁdently modify / complete the body of a Python function to make the code perform the algorithm. correctly.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Why not R?\\n\\nI don’t know R. The machine learning community mostly uses Python. Follow-up courses like STA414 typically use Python.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n6 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Course Information\\n\\nMost information: website is main source of information; check regularly!\\n\\nhttps://www.cs.toronto.edu/~cmaddis/courses/sta314_f21/', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Announcements, grades, & links: Quercus.\\n\\nDid you receive the announcement?\\n\\nDiscussions: Piazza.\\n\\nSign up: https://piazza.com/utoronto.ca/fall2021/sta314 Your grade does not depend on your participation on Piazza. It’s a good way for asking questions, discussing with the course community. Only discuss course materials/assignments/exams, but do not give homework hints.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n7 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Course Information\\n\\nDaily class schedule: All sections (LEC0101 and LEC0201) have the same basic schedule.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '1hr Oﬃce Hour\\n\\nZoomClass ComponentDelivery Method\\n\\nZoom\\n\\n2hr Lecture\\n\\nZoom /in-person\\n\\n1hr Tutorial\\n\\nMore detailed delivery instructions on the next slide.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n8 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Course Information\\n\\nDelivery instructions:', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'All lectures and oﬃce hours are synchronous via Zoom. Tutorials are synchronous via Zoom and potentially in-person. First 2 weeks, all tutorials are synchronous via Zoom. Afterwards, we will determine the demand and appropriateness for in-person tutorials. Whether your tutorial is in-person or online depends on your enrollment (see website for locations). If you are enrolled in an in-person tutorial, but prefer online, you can join the online tutorial for your section. Do not attend an in-person', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'tutorial unless you are enrolled in one.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Oﬃce hours are not mandatory.\\n\\nWeek 5 will be fully asynchronous, because of Thanksgiving.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n9 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Course Information\\n\\nLectures will be delivered synchronously via Zoom, and recorded for asynchronous viewing by enrolled students. All information about attending virtual lectures, tutorials, and oﬃce hours will be sent to enrolled students through Quercus.\\n\\nYou may download recorded lectures for your own academic use, but you should not copy, share, or use them for any other purpose.\\n\\nDuring lecture, please keep yourself on mute unless called upon.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'In case of illness, you should ﬁll out the absence declaration form on ACORN and notify the instructors to request special consideration.\\n\\nFor accessibility services: If you require additional academic accommodations, please contact UofT Accessibility Services as soon as possible, studentlife.utoronto.ca/as. There is a volunteer note-taker in the course, if you need this service. Check Quercus for details.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n10 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Course Information\\n\\nAll that information is in the STA314 syllabus and on the website. If you remember just one thing:', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Check Quercus and website regularly.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n11 / 65\\n\\nSuggested Readings\\n\\nSuggested readings will be given for each lecture. These are completely optional, but useful. The following will be useful throughout the course:\\n\\nHastie, Tibshirani, and Friedman. The Elements of Statistical Learning.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Christopher Bishop. Pattern Recognition and Machine Learning.\\n\\nKevin Murphy. Machine Learning: a Probabilistic Perspective.\\n\\nDavid MacKay. Information Theory, Inference, and Learning Algorithms. Shai Shalev-Shwartz & Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'David Barber. Bayesian Reasoning and Machine Learning.\\n\\nThere are lots of freely available, high-quality ML resources.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n12 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Marking\\n\\n(60%) 4 assignments\\n\\nCombination of pen & paper derivations and light-weight programming exercises. Weighted equally. Hand-in on Quercus.\\n\\n(40%) Two 1-hour tests held during normal class time\\n\\nSee website for times and dates. Weighted equally. Taken on Quercus.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n13 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'More on Assignments\\n\\nStudents may work on the assignments alone or in pairs. The marking scheme will not be adapted depending on the size of the team. If you choose to work in a pair, then you:\\n\\nMust register the pair with the instructur before the due date.\\n\\nMust include a contributions statement in your submission that describes the contribution of each team member.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'You must not share proofs, pseudocode, code, or simulation results with students that are not your teammates. Violation of this policy is an academic oﬀence and will be investigated and reported as such.\\n\\nAssignments should be handed in by deadline; a late penalty of 10% of the total credit of the assignment per day will be assessed thereafter (up to 3 days, then submission is blocked).', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Extensions will be granted only in special situations, and you will need to complete an absence declaration form and notify us to request special consideration, or otherwise have a written request approved by the course instructors at most one week after the due date.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n14 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'What is the diﬀerence between this course and CSC311?\\n\\nIn the past, STA314 and CSC311 have been taught diﬀerently, despite being exclusions.\\n\\nThis year, I am bringing them in line with each other.\\n\\nHere are major diﬀerences this year:\\n\\nWe do not cover neural networks nor reinforcement learning. We cover the probabilistic interpretation in a bit more detail. The emphasis in the homeworks is more on proofs and less on coding.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'In other words, STA314 takes a more statistical perspective than CSC311 while covering the same core of material.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n15 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Advanced Courses\\n\\nThis course will help prepare you for the following courses.\\n\\nSTA414 (Statistical Methods for Machine Learning II)\\n\\nThis course is the follow-up course, which delves deeper into the probabilistic interpretation of machine learning that we cover in the last few weeks.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'CSC413 (Neural Networks and Deep Learning)\\n\\nThis course covers deep learning and automatic diﬀerentiation.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'CSC412 (Probabilistic Learning and Reasoning)\\n\\nThe CSC analogue of STA414.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n16 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nQuestions?\\n\\nSTA314-Lec1\\n\\n17 / 65\\n\\nWhat is learning?\\n\\n”The activity or process of gaining knowledge or skill by studying, practicing, being taught, or experiencing something.”', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Merriam Webster dictionary\\n\\n“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Tom Mitchell\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n18 / 65\\n\\nWhat is machine learning?\\n\\nFor many problems, it’s diﬃcult to program the correct behavior by hand\\n\\nrecognizing people and objects understanding human speech\\n\\nMachine learning approach: program an algorithm to automatically learn from data, or from experience Why might you want to use a learning algorithm?', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'hard to code up a solution by hand (e.g. vision, speech) system needs to adapt to a changing environment (e.g. spam detection) want the system to perform better than the human programmers privacy/fairness (e.g. ranking search results)\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n19 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Relations to statistics\\n\\nIt’s similar to statistics...\\n\\nBoth ﬁelds try to uncover patterns in data Both ﬁelds draw heavily on calculus, probability, and linear algebra, and share many of the same core algorithms\\n\\nit’s not exactly statistics...\\n\\nStats is more concerned with helping scientists and policymakers draw rigorous conclusions about data ML is more concerned with making predictions that are very accurate\\n\\nand the communities are somewhat diﬀerent...', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Stats puts more emphasis on interpretability and mathematical rigor ML puts more emphasis on predictive performance, scalability, and autonomy\\n\\n...but machine learning and statistics rely on similar mathematics.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n20 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Relations to AI\\n\\nNowadays, “machine learning” is often brought up with “artiﬁcial intelligence” (AI)\\n\\nAI does not always imply a learning based system\\n\\nSymbolic reasoning Rule based system Tree search etc.\\n\\nLearning based system → learned based on the data → more ﬂexibility, good at solving pattern recognition problems.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n21 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Relations to human learning\\n\\nHuman learning is:\\n\\nVery data eﬃcient An entire multitasking system (vision, language, motor control, etc.) Takes at least a few years :)\\n\\nFor serving speciﬁc purposes, machine learning doesn’t have to look like human learning in the end.\\n\\nIt may borrow ideas from biological systems, e.g., neural networks.\\n\\nIt may perform better or worse than humans.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n22 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Types of machine learning\\n\\nSupervised Learning\\n\\nUnsupervised Learning\\n\\nMachine is given data and examples of what to predict.\\n\\nMachine is given data, but not what to predict.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Reinforcement Learning\\n\\nMachine gets data by interacting with an environment and tries to minimize a cost.\\n\\n23 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'History of machine learning\\n\\n1957 — Perceptron algorithm (implemented as a circuit!)\\n\\n1959 — Arthur Samuel wrote a learning-based checkers program that could defeat him\\n\\n1969 — Minsky and Papert’s book Perceptrons (limitations of linear models) 1980s — Some foundational ideas', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Connectionist psychologists explored neural models of cognition 1984 — Leslie Valiant formalized the problem of learning as PAC learning 1988 — Backpropagation (re-)discovered by Geoﬀrey Hinton and colleagues 1988 — Judea Pearl’s book Probabilistic Reasoning in Intelligent Systems introduced Bayesian networks\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n24 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'History of machine learning\\n\\n1990s — the “AI Winter”, a time of pessimism and low funding But looking back, the ’90s were also sort of a golden age for ML research\\n\\nMarkov chain Monte Carlo variational inference kernels and support vector machines boosting convolutional networks reinforcement learning\\n\\n2000s — applied AI ﬁelds (vision, NLP, etc.) adopted ML 2010s — deep learning', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '2010–2012 — neural nets smashed previous records in speech-to-text and object recognition increasing adoption by the tech industry 2016 — AlphaGo defeated the human Go champion 2018-now — generating photorealistic images and videos 2020 — GPT3 language model\\n\\nnow — increasing attention to ethical and societal implications\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n25 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Computer vision: Object detection, semantic segmentation, pose estimation, and almost every other task is done with ML.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Instance segmentation -\\n\\nLink\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n26 / 65\\n\\nSpeech: Speech to text, personal assistants, speaker identiﬁcation...\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n27 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'NLP: Machine translation, sentiment analysis, topic modeling, spam ﬁltering.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n28 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Playing Games\\n\\nDOTA2 -\\n\\nLink\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n29 / 65\\n\\nRecommender Systems : Amazon, Netﬂix, ...\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n30 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Why this class?\\n\\n“I’ve heard that neural networks solve everything, can we just learn those?”\\n\\nThe principles you learn in this course will be essential to understand and apply neural nets. The techniques in this course are still the ﬁrst things to try for a new ML problem.\\n\\nE.g., try logistic regression before building a deep neural net!\\n\\nThere’s a whole world of probabilistic graphical models (STA414).\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n31 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Why this class?\\n\\n2017 Kaggle survey of data science and ML practitioners: what data science methods do you use at work?\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n32 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ML Workﬂow\\n\\nML workﬂow sketch:\\n\\n1 Should I use ML on this problem?\\n\\nIs there a pattern to detect? Can I solve it analytically? Do I have data? 2 Gather and organize data.\\n\\nPreprocessing, cleaning, visualizing.\\n\\n3 Establishing a baseline, i.e., implement the simplest, default ML', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'method.\\n\\n4 Choose and tailor an ML method to your problem.\\n\\n5 Analyze performance & mistakes, and iterate.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n33 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Implementing machine learning systems\\n\\nA key part of this workﬂow involves modifying existing methods to make them ﬁt your setting.\\n\\nThis will often involve deriving an algorithm (with pencil and paper), and then translating the math into code.\\n\\nOne of the key ideas in machine learning is array processing or vectorized computations, i.e., express the algorithm in terms of matrix/vector operations to exploit hardware eﬃciency (more in next week’s tutorial on NumPy).\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '34 / 65\\n\\nImplementing machine learning systems\\n\\nThere are now very convenient and powerful frameworks: PyTorch, TensorFlow, JAX, etc.\\n\\nautomatic diﬀerentiation compiling computation graphs libraries of algorithms and network primitives support for graphics processing units (GPUs)\\n\\nWhy take this class if these frameworks do so much for you?', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'So you know what to do if something goes wrong! Debugging learning algorithms requires sophisticated detective work, which requires understanding what goes on beneath the hood. That’s why we derive things by hand in this class!\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n35 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nPreliminaries and Nearest Neighbor Methods\\n\\nSTA314-Lec1\\n\\n36 / 65\\n\\nIntroduction\\n\\nToday (and for much of this course) we focus on supervised learning.\\n\\nFor many tasks of interest we are given some data and are interested in predicting something about that data.\\n\\nTask object recognition image captioning document classiﬁcation speech-to-text ...', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Input data image image text audio waveform ...\\n\\nWe wish to predict object category caption document category text ...\\n\\nSupervised learning is applicable when we have many examples of good predictions, i.e., we can supervise the learner by telling it exactly what to predict.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n37 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Introduction\\n\\nMore precisely, in supervised learning, we are given\\n\\ntraining set consisting of\\n\\ninputs x and corresponding', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'labels t.\\n\\nOur goal is to predict the label or learn the mapping from x → t. Let’s unpack this carefully.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n38 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Input Vectors\\n\\nLet’s consider image input data. What an image looks like to the computer:\\n\\n[Image credit: Andrej Karpathy]\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n39 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Input Vectors\\n\\nMachine learning algorithms need to handle lots of types of data: images, text, audio waveforms, credit card transactions, etc.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Common strategy: represent the input as an input vector in Rd\\n\\nRepresentation = mapping to another space that’s easy to manipulate Vectors are a great representation since we can do linear algebra!\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n40 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Input Vectors\\n\\nCan use raw pixels:\\n\\nCan do much better if you compute a vector of meaningful features.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n41 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Labels\\n\\nYou can think of labels as answers to questions about the data.\\n\\ne.g., what is the ambient temperature in the picture? e.g., what is the primary object in the image?\\n\\nWe can use numbers to represent labels as well. Traditionally, we use diﬀerent names depending on the type of label t.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Regression: t is a real number, e.g., the ambient temperature Classiﬁcation: t ∈ {1, . . . , C } is an element of a discrete set, e.g., let 1 mean “dog”, 2 mean “cat”, etc. Structured prediction: these days, t is often a highly structured object (e.g., image can also be a label)\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n42 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Training sets\\n\\nTo summarize, mathematically, our training set consists of a collection of pairs of an input x ∈ Rd and its corresponding label t.\\n\\nDenote the training set {(x (1), t (1)), . . . , (x (N), t (N))}\\n\\nNote: these superscripts have nothing to do with exponentiation!\\n\\nOur goal is to learn a mapping from x (i) → t (i) that performs well (will be more precise about this later).\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n43 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Nearest Neighbors\\n\\nSuppose we’re given a novel input vector x we’d like to classify.\\n\\nThe idea: ﬁnd the nearest input vector to x in the training set and copy its label.\\n\\nCan formalize “nearest” in terms of Euclidean distance\\n\\n||x (a) − x (b)||2 =\\n\\n(cid:118) (cid:117) (cid:117) (cid:116)\\n\\nd (cid:88)\\n\\n(x (a)', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'j − x (b)\\n\\nj\\n\\n)2\\n\\nj=1\\n\\nAlgorithm:\\n\\n1 Find example (x ∗, t ∗) (from the stored training set) closest to x.\\n\\nThat is:', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'x ∗ = arg\\n\\nmin x (i)∈train. set\\n\\ndistance(x (i), x)\\n\\n2 Output y = t ∗\\n\\nNote: we don’t need to compute the square root. Why?\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n44 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Nearest Neighbors: Decision Boundaries\\n\\nWe can visualize the behavior in the classiﬁcation setting using a Voronoi diagram.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n45 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Nearest Neighbors: Decision Boundaries\\n\\nDecision boundary: the boundary between regions of input space assigned to diﬀerent categories.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n46 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Nearest Neighbors: Decision Boundaries\\n\\nExample: 2D decision boundary\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n47 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Nearest Neighbors\\n\\nNearest neighbors sensitive to noise or mis-labeled data (“class noise”).\\n\\nSolution? Smooth by having k nearest neighbors vote\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n48 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'k-Nearest Neighbors\\n\\nAlgorithm (kNN):\\n\\n1 Find k examples {x (i), t (i)} closest to the test instance x\\n\\n2 Classiﬁcation output is majority class', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'y = arg max t(z)\\n\\nk (cid:88)\\n\\ni=1\\n\\nI(t (z) = t (i))\\n\\nI{statement} = 1 when the statement is true, and 0 otherwise.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n49 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'k-NN\\n\\nk=1\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n[Image credit: ”The Elements of Statistical Learning”]\\n\\n50 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'k-NN\\n\\nk=15\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n[Image credit: ”The Elements of Statistical Learning”]\\n\\n51 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'k-NN\\n\\nTradeoﬀs in choosing k?\\n\\nSmall k\\n\\nGood at capturing ﬁne-grained patterns May be sensitive to random idiosyncrasies in the training data, we call this overﬁtting.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Large k\\n\\nMakes stable predictions by averaging over lots of examples May fail to capture important regularities, we call this underﬁtting.\\n\\nBalancing k\\n\\nOptimal choice of k depends on number of data points n. Nice theoretical properties if k → ∞ and k Rule of thumb: choose k <\\n\\nn → 0 (ESL 2.4).\\n\\n√', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'n.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n52 / 65\\n\\nGeneralization error for k-NN\\n\\nWe would like our algorithm to generalize to data it hasn’t seen before.\\n\\nHow can we measure the generalization error (error rate on new examples)?\\n\\nUse a new, unseen set of input-label pairs called a test set.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n[Image credit: ”The Elements of Statistical Learning”]\\n\\n53 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Hyperparameters\\n\\nMaybe we can use the test set to pick k? However, once we use the test set of pick k, it is no longer an unbiased way of measuring of how well we will do on unseen data.\\n\\nk is an example of a hyperparameter, something we can’t ﬁt as part of the learning algorithm itself.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n54 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Validation sets\\n\\nWe can tune hyperparameters using a validation set, which is a third, unseen set of input-label pairs.\\n\\nThe test set is used only at the very end, to measure the generalization performance of the ﬁnal conﬁguration.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n55 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Pitfalls of k-NN: The Curse of Dimensionality\\n\\nLow-dimensional visualizations are misleading!\\n\\nIn high dimensions, “most” points are far apart.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n56 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Pitfalls of k-NN: The Curse of Dimensionality\\n\\nSuppose we want we want the nearest neighbor of every query x ∈ [0, 1]d to be closer than (cid:15), how many points do we need in our training set to guarantee it?\\n\\nThe volume of a single ball of radius (cid:15) around each point is O((cid:15)d ) The total volume of [0, 1]d is 1. O (cid:0)( 1 exponentially in d.\\n\\n(cid:15) )d (cid:1) points are needed to cover the volume, i.e., increasing\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '[Image credit: ”The Elements of Statistical Learning”]\\n\\n57 / 65\\n\\nPitfalls: The Curse of Dimensionality\\n\\nIn high dimensions, “most” points are approximately the same distance.\\n\\nWe can show this by applying the rules of expectation and covariance of random variables in surprising ways.\\n\\nPicture to keep in mind:\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n58 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Pitfalls: The Curse of Dimensionality\\n\\nSaving grace: some datasets (e.g. images) may have low intrinsic dimension, i.e. lie on or near a low-dimensional manifold.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Image credit: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html\\n\\nThe neighborhood structure (and hence the Curse of Dimensionality) depends on the intrinsic dimension. The space of megapixel images is 3 million-dimensional. The true number of degrees of freedom is much smaller.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n59 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Pitfalls: Normalization\\n\\nNearest neighbors can be sensitive to the ranges of diﬀerent features.\\n\\nOften, the units are arbitrary:\\n\\nSimple ﬁx: normalize each dimension to be zero mean and unit variance. I.e., compute the mean µj and standard deviation σj , and take\\n\\n˜x (i) j =', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'x (i) j − µj σj\\n\\nCaution: depending on the problem, the scale might be important!\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n60 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Pitfalls: Computational Cost\\n\\nNumber of computations at training time: 0', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Number of computations at test time, per query (na¨ıve algorithm)\\n\\nCalculuate D-dimensional Euclidean distances with N data points: O(ND) Sort the distances: O(N log N)\\n\\nThis must be done for each query, which is very expensive by the standards of a learning algorithm!\\n\\nNeed to store the entire dataset in memory!\\n\\nTons of work has gone into algorithms and data structures for eﬃcient nearest neighbors with high dimensions and/or large datasets.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n61 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Example: Digit Classiﬁcation\\n\\nDecent performance when lots of data\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n[Slide credit: D. Claus]\\n\\n62 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Example: Digit Classiﬁcation\\n\\nChanging the similarity measure can really improve k-NN.\\n\\nExample: shape contexts for object recognition. In order to achieve invariance to image transformations, they tried to warp one image to match the other image.\\n\\nDistance measure: average distance between corresponding points on warped images\\n\\nAchieved 0.63% error on MNIST, compared with 3% for Euclidean KNN.\\n\\nCompetitive with the state of the art at the time, but required careful engineering.', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '[Belongie, Malik, and Puzicha, 2002. Shape matching and object recognition using shape contexts.]\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n63 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Conclusions\\n\\nSimple algorithm that does all its work at test time — in a sense, no learning!\\n\\nCan control the complexity by varying k', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Suﬀers from the Curse of Dimensionality\\n\\nNext time: parametric models, which learn a compact summary of the data rather than referring back to it at test time.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n64 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nQuestions?\\n\\nSTA314-Lec1\\n\\n65 / 65', 'filename': 'data/coursematerial/test_data/lec01.pdf'}\n",
      "Uploading chunk\n",
      "Uploading 108 chunks for data/coursematerial/test_data/lec03.pdf.\n",
      "{'content': 'STA 314: Statistical Methods for Machine Learning I Lecture 3 - Bias-Variance Decomposition\\n\\nChris J. Maddison', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'University of Toronto\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n1 / 29\\n\\nToday\\n\\nExpand a bit on Q3 and Q4 of the HW1.\\n\\nToday we will talk about the bias-variance decomposition, which is beginning to make more precise our discussion of overﬁtting and underﬁtting last class.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n2 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Q3, HW1\\n\\nN i=1 of xi }\\n\\nGiven any ﬁnite set N i=1, which is any D such that variable over }\\n\\nxi {\\n\\nR, we can deﬁne the uniform random\\n\\n∈', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'xi {\\n\\nP(D = xi ) =\\n\\n1 N\\n\\nSampling from this random variable is easy: sample an integer J\\n\\n1, . . . , N', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'uniformly at random and return xJ .\\n\\n} For this distribution, we have\\n\\n∈ {\\n\\nE[D] =\\n\\nN\\n\\ni=1 (cid:88)\\n\\nP(D = xi )xi =', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'N\\n\\ni=1 (cid:88)\\n\\n1 N\\n\\nxi\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n3 / 29\\n\\nRecall: supervised learning\\n\\nIn supervised learning, our learning algorithms (k-NN, decision trees) produce predictions ˆy (cid:63)(x)', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 't for a query point x.\\n\\n≈\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n4 / 29\\n\\nRecall: supervised learning\\n\\nWe can think of this as picking a predictor function ˆy (cid:63) from a hypothesis class by minimizing the average loss on the training set', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '∈ H\\n\\nˆy (cid:63) = arg min y ∈H\\n\\nˆ R\\n\\n[y ,\\n\\nD\\n\\ntrain]\\n\\nThen, we measure the average loss on an unseen test set to approximate how well ˆy (cid:63) does on the true data generating distribution,', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ˆ R\\n\\n[ˆy (cid:63),\\n\\nD\\n\\ntest]\\n\\n≈ R\\n\\n[ˆy (cid:63)]\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n5 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Recall: supervised learning\\n\\nThis view of supervise learning is a very idealized view:\\n\\n(cid:73) k-NN algorithm for k > 1 doesn’t really select the predictor by\\n\\nminimizing a global loss.\\n\\n(cid:73) Decision tree ﬁtting does select ˆy (cid:63) based on training loss, but it is often greedy and sometimes does not ﬁnd the global optimal ˆy (cid:63).\\n\\nStill, it’s a very useful general model for supervised learning.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n6 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Q4, HW1\\n\\nLet’s consider Q4 in HW1 as a way to review this supervised framewrok.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n7 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bias-Variance Decomposition\\n\\nRecall that overly simple hypothesis classes underﬁt the data, and overly complex ones overﬁt.\\n\\nLast lecture we talked about this intuitively.\\n\\nWe can quantify this eﬀect in terms of the bias-variance decomposition.\\n\\n(cid:73) So far we’ve been talking about the training set as if it is ﬁxed, but it\\n\\nmakes more sense to think of it as random.\\n\\n(cid:73) So, we’d like to understand how our learning algorithm is impacted by', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'selecting a predictor on a ﬁnite, random, training set.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n8 / 29\\n\\nBias-Variance Decomposition: Basic Setup\\n\\n(x(i), t (i)) Recall: the training set a single data generating distribution pdata. Consider a ﬁxed query point x (green x below).', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'train =\\n\\nD\\n\\n{\\n\\nN i=1 contains N i.i.d. draws from }\\n\\nConsider sampling many training sets\\n\\ntrain n D', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'independently from pdata.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n9 / 29\\n\\nBias-Variance Decomposition: Basic Setup', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'train n D\\n\\n, run learning alg. to get a predictor ˆy (cid:63)\\n\\nFor each training set\\n\\nn ∈ H n (x) and compare it to a label t drawn from\\n\\nCompute the prediction ˆy (cid:63) pdata(t\\n\\nx). | We can view ˆy (cid:63) the choice of training set.\\n\\nn as a random variable, where the randomness comes from\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n.\\n\\n10 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bias-Variance Decomposition: Basic Setup\\n\\nHere is the analogous setup for regression:\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\nSince y is a random variable, we can talk about its expectation, variance, etc.\\n\\n11 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bias-Variance Decomposition: Basic Setup\\n\\nRecap of basic setup:\\n\\n(cid:73) Fix a query point x. (cid:73) Sample the (true) target t from the conditional distribution pdata(t (cid:73) Repeat:\\n\\nx). |\\n\\n(cid:73) Sample a random training dataset Dtrain', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'n\\n\\ni.i.d. from the data generating\\n\\ndistribution pdata.\\n\\n(cid:73) Run the learning algorithm on Dtrain', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'n\\n\\nto get a prediction ˆy (cid:63)\\n\\nn (x) from H\\n\\nat x.\\n\\n(cid:73) Compute the loss L(ˆy (cid:63)\\n\\nn (x), t).\\n\\n(cid:73) Average the losses.\\n\\nNotice: y is independent of t given x.\\n\\nThis gives a distribution over the loss at x, with expectation E[L(ˆy (cid:63)(x), t) ˆy (cid:63) = arg miny ∈H ˆ R\\n\\nx] taken over t and the random training set\\n\\n|', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'train].\\n\\n[y ,\\n\\nD\\n\\nD\\n\\ntrain where\\n\\nFor each query point x, the expected loss is diﬀerent. We are interested in minimizing the expectation of this with respect to x', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'pdata(x).\\n\\n∼\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n12 / 29\\n\\nBayes Optimality\\n\\nFor now, focus on squared error loss, L(y , t) = 1\\n\\n2 (y\\n\\n−', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 't)2 with y , t\\n\\nA ﬁrst step: suppose we knew the conditional distribution pdata(t is the best deterministic value y (x)\\n\\nR should we predict?\\n\\n∈ (cid:73) Here, we are treating t as a random variable and choosing y (x).\\n\\nClaim: y (cid:63)(x) = E[t Proof: Consider a ﬁxed y\\n\\nx] is the best possible prediction.\\n\\n|', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'R\\n\\n∈ x] = E[y 2 = y 2 = y 2 = y 2\\n\\nt)2\\n\\n2yt + t 2\\n\\nE[(y\\n\\nx] | − x] + E[t 2 2y E[t 2y E[t x] + E[t 2yy (cid:63)(x) + y (cid:63)(x)2 + Var[t y (cid:63)(x))2 + Var[t\\n\\n−\\n\\n|\\n\\nx] | x]2 + Var[t\\n\\n−\\n\\n|\\n\\n|\\n\\n|\\n\\n−\\n\\n−\\n\\n|', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'x]\\n\\n= (y\\n\\n|\\n\\n−\\n\\n| x]\\n\\nx]\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n|\\n\\nR. ∈ x). What\\n\\n13 / 29\\n\\nBayes Optimality\\n\\nE[(y\\n\\n−\\n\\nt)2\\n\\n|\\n\\nx] = (y\\n\\n−\\n\\ny (cid:63)(x))2 + Var[t\\n\\n|', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'x]\\n\\nThe ﬁrst term is nonnegative, and can be made 0 by setting y = y (cid:63)(x).\\n\\nThe second term corresponds to the inherent unpredictability, or noise, of the targets, and is called the Bayes error.\\n\\n(cid:73) This is the best we can ever hope to do with any learning algorithm.\\n\\nAn algorithm that achieves it is Bayes optimal.\\n\\n(cid:73) Notice that this term doesn’t depend on y .\\n\\nThis process of choosing a single value y (cid:63)(x) based on pdata(t example of decision theory.\\n\\n|\\n\\nx) is an', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nSTA314-Lec1\\n\\n14 / 29\\n\\nBayes Optimality\\n\\nBut, in practice, our prediction ˆy (cid:63)(x) is not y (cid:63)(x). Instead, it is a random variable (where the randomness comes from randomness of the training set) taking values in\\n\\n.', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H\\n\\nWe can decompose out the expected loss.\\n\\nSuppressing the dependence on x for clarity:\\n\\nE[(ˆy (cid:63)\\n\\n−\\n\\nt)2] = E[(ˆy (cid:63) = E[y (cid:63)2 = y (cid:63)2 = y (cid:63)2 = (y (cid:63)\\n\\ny (cid:63))2] + Var(t) 2y (cid:63) ˆy (cid:63) + ˆy (cid:63)2] + Var(t)\\n\\n−\\n\\n− 2y (cid:63)\\n\\nE[ˆy (cid:63)] + E[ˆy (cid:63)2] + Var(t) E[ˆy (cid:63)] + E[ˆy (cid:63)]2 + Var(ˆy (cid:63)) + Var(t)\\n\\n−\\n\\n2y (cid:63) E[ˆy (cid:63)])2 bias\\n\\n−', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '+ Var(ˆy (cid:63))\\n\\n+ Var(t)\\n\\n−\\n\\nvariance\\n\\nBayes error\\n\\n(cid:124)\\n\\n(cid:123)(cid:122)\\n\\n(cid:125)\\n\\n(cid:124) (cid:123)(cid:122) (cid:125)\\n\\n(cid:124) (cid:123)(cid:122) (cid:125)\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n15 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bayes Optimality\\n\\nLet’s step back and consider what we just did. First, recall:\\n\\n(cid:73) Picking a predictor by minimizing the average loss on the training set\\n\\nˆy (cid:63) = arg min y ∈H', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ˆ R\\n\\n[y ,\\n\\nD\\n\\ntrain]\\n\\nreturns a random predictor ˆy (cid:63).\\n\\n(cid:73) But, we’re interested in our performance in terms of expected loss:', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'R\\n\\n[ˆy (cid:63)]\\n\\nIn our case:\\n\\nR\\n\\n[ˆy (cid:63)] = E\\n\\nE[(ˆy (cid:63)(x)\\n\\n−\\n\\nt)2\\n\\n|\\n\\nx]\\n\\n.\\n\\n(cid:2)\\n\\n(cid:3)\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n16 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bayes Optimality\\n\\nE\\n\\n(cid:2)\\n\\nE[(ˆy (cid:63)(x)\\n\\n−\\n\\nt)2\\n\\n|\\n\\nx]\\n\\n(cid:3)\\n\\n= E \\uf8ee\\n\\n(y (cid:63)(x)\\n\\n\\uf8ef \\uf8f0 (cid:124)\\n\\n−', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'E[ˆy (cid:63)(x) bias\\n\\n(cid:123)(cid:122)\\n\\n|\\n\\nx])2\\n\\n(cid:125)\\n\\n+ Var[ˆy (cid:63)(x)\\n\\nvariance\\n\\n(cid:124)\\n\\n(cid:123)(cid:122)', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '[ˆy (cid:63)] into three terms:\\n\\nSo, we just split the expected loss\\n\\nR\\n\\n(cid:73) bias: how wrong the expected prediction is (cid:73) variance: the amount of variability in the predictions (cid:73) Bayes error: the inherent unpredictability of the targets\\n\\nHow does our choice of', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H\\n\\ninteract with this analysis?\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n|\\n\\nx]\\n\\n(cid:125)\\n\\n+ Var[t\\n\\n|\\n\\nx]\\n\\n\\uf8f9\\n\\nBayes error\\n\\n(cid:124)\\n\\n(cid:123)(cid:122)\\n\\n(cid:125)\\n\\n\\uf8fa \\uf8fb\\n\\n17 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '382.OverviewofSupervisedLearning', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'N!i(yi−ˆyi)2.Unfortunatelytrainingerrorisnotagoodestimateoftesterror,asitdoesnotproperlyaccountformodelcomplexity.Figure2.11showsthetypicalbehaviorofthetestandtrainingerror,asmodelcomplexityisvaried.Thetrainingerrortendstodecreasewheneverweincreasethemodelcomplexity,thatis,wheneverweﬁtthedataharder.Howeverwithtoomuchﬁtting,themodeladaptsitselftoocloselytothetrainingdata,andwillnotgeneralizewell(i.e.,havelargetesterror).Inthatcasethepredictionsˆf(x0)willhavelargevariance,asreﬂectedinthelasttermof', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'expression(2.46).Incontrast,ifthemodelisnotcomplexenough,itwillunderﬁtandmayhavelargebias,againresultinginpoorgeneralization.InChapter7wediscussmethodsforestimatingthetesterrorofapredictionmethod,andhenceestimatingtheoptimalamountofmodelcomplexityforagivenpredictionmethodandtrainingset.', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'HighBiasLowVarianceLowBiasHighVariancePredictionErrorModelComplexityTrainingSampleTestSampleLowHighFIGURE2.11.Testandtrainingerrorasafunctionofmodelcomplexity.beclosetof(x0).Askgrows,theneighborsarefurtheraway,andthenanythingcanhappen.Thevariancetermissimplythevarianceofanaveragehere,andde-creasesastheinverseofk.Soaskvaries,thereisabias–variancetradeoﬀ.Moregenerally,asthemodelcomplexityofourprocedureisincreased,thevariancetendstoincreaseandthesquaredbiastendstodecrease.Theop-positebehavioroccurs', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'asthemodelcomplexityisdecreased.Fork-nearestneighbors,themodelcomplexityiscontrolledbyk.Typicallywewouldliketochooseourmodelcomplexitytotradebiasoﬀwithvarianceinsuchawayastominimizethetesterror.Anobviousestimateoftesterroristhetrainingerror1', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bayes Optimality\\n\\nSource: ESL\\n\\nis large, then ˆy (cid:63) can get close y (cid:63), therefore reducing bias. It’s also\\n\\nIf sensitive to the ﬁnite training set, therefore increasing variance.', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H\\n\\nis small, then ˆy (cid:63) is typically from y (cid:63), therefore increasing bias. It’s less\\n\\nIf sensitive to the ﬁnite training set, therefore reducing variance.', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H\\n\\nEven though this analysis only applies to squared error, we often loosely use “bias” and “variance” as synonyms for “underﬁtting” and “overﬁtting”.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n18 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bias and Variance\\n\\nThrowing darts = predictions for each draw of a dataset\\n\\nSource: ESL.\\n\\nBe careful, the expected loss averages over points x from the data distribution, so this produces its own type of variance.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n19 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bias/Variance Decomposition: Another Visualization\\n\\nIn practice, measure the average loss ˆ R\\n\\n[ˆy (cid:63),\\n\\ntest] on the test set instead of', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'D\\n\\n[ˆy (cid:63)].\\n\\nR Let’s visualize the bias-variance decomposition by plotting the space of predictions of the model, where each axis correspond to predictions on a two test examples (x(1), x(2)).\\n\\ncontours of expected loss', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"A670NCeixZigUUibfD3V4OURW7k=\">AAACDnicbVDLTgIxFL3jE/GFunTTSExwQ2YIUZYkblxiIo8ERtLpFGjodCZthziZ8A+61f9wZ9z6C/6GX2CBWQh4kiYn59ybe3q8iDOlbfvb2tjc2t7Zze3l9w8Oj44LJ6ctFcaS0CYJeSg7HlaUM0GbmmlOO5GkOPA4bXvj25nfnlCpWCgedBJRN8BDwQaMYG2kdvKYlipX036haJftOdA6cTJShAyNfuGn54ckDqjQhGOluo4daTfFUjPC6TTfixWNMBnjIe0aKnBAlZvO407RpVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwbzpiRntZJ10qqUnety9b5arNeyunJwDhdQAgduoA530IAmEBjDC7zCm/VsvVsf1udidMPKds5gC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwy0nNE=</latexit>y(2)<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"PHUxT+Vwih3GhA51ADl0JcRjAgk=\">AAACDnicbVDLTgIxFL2DL8QX6tJNIzHBDZkxRFmSuHGJiTwSGEmnU6Ch05m0HeJkwj/oVv/DnXHrL/gbfoEFZiHgSZqcnHNv7unxIs6Utu1vK7exubW9k98t7O0fHB4Vj09aKowloU0S8lB2PKwoZ4I2NdOcdiJJceBx2vbGtzO/PaFSsVA86CSiboCHgg0YwdpI7eQxLTuX036xZFfsOdA6cTJSggyNfvGn54ckDqjQhGOluo4daTfFUjPC6bTQixWNMBnjIe0aKnBAlZvO407RhVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwYLpiRntZJ10rqqONeV6n21VK9ldeXhDM6hDA7cQB3uoAFNIDCGF3iFN+vZerc+rM/FaM7Kdk5hC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwsOnNA=</latexit>y(1)', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"wWE2msvdeLociE0Le8HWYcmTxVg=\">AAACH3icbZDLSsNAFIYnXmu9RV26GSxCClKSUmqXBTcuK9gLNGmZTCbt0MmFmYlYQt7Eja/ixoUi4q5v47TNwrYeGPj5/nNmzvxuzKiQpjnTtrZ3dvf2CwfFw6Pjk1P97LwjooRj0sYRi3jPRYIwGpK2pJKRXswJClxGuu7kbu53nwgXNAof5TQmToBGIfUpRlKhoV5P7cUlKSdeBo3pILWFRDwzngepYZWz8g1cZVXFytlQL5kVc1FwU1i5KIG8WkP9x/YinAQklJghIfqWGUsnRVxSzEhWtBNBYoQnaET6SoYoIMJJF6tl8FoRD/oRVyeUcEH/TqQoEGIauKozQHIs1r05/M/rJ9JvOCkN40SSEC8f8hMGZQTnYUGPcoIlmyqBMKdqV4jHiCMsVaRFFYK1/uVN0alWrHql9lArNRt5HAVwCa6AASxwC5rgHrRAG2DwAt7AB', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '/jUXrV37Uv7XrZuafnMBVgpbfYL8OCiNg==</latexit>(y?(x(1)),y?(x(2)))<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"EQgY3A2OxWGSfXUfnrQV1cSnQIQ=\">AAAB+3icbZDLSsNAFIZPvNZ6i3XpZrAILUhJStEuC25cVrAXaGOZTCft0MmFmYlYQl7FjQtF3Poi7nwbJ20W2vrDwMd/zuGc+d2IM6ks69vY2Nza3tkt7BX3Dw6Pjs2TUleGsSC0Q0Ieir6LJeUsoB3FFKf9SFDsu5z23NlNVu89UiFZGNyreUQdH08C5jGClbZGZqmiHpKKXU0vUQb1alodmWWrZi2E1sHOoQy52iPzazgOSezTQBGOpRzYVqScBAvFCKdpcRhLGmEywxM60Bhgn0onWdyeogvtjJEXCv0ChRbu74kE+1LOfVd3+lhN5WotM/+rDWLlNZ2EBVGsaECWi7yYIxWiLAg0ZoISxecaMBFM34rIFAtMlI6rqEOwV7+8Dt16zb6qNe4a5VYzj6MAZ3AOFbDhGlpwC23oAIEneIZXeDNS48V4Nz6WrRtGPnMKf2R8/', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'gAwfZKU</latexit>(t(1),t(2))<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"KDsnwBDevF3vsowUKo/TwTh4UF4=\">AAACcHicjVFdSyMxFE1Hd9Xuh1VfBB82u2WhXaTMiKiPgg/6pgtWhU4tmfS2DWYyQ3KnWEJ+4v6A/Rv6qmDazsL68eCFwOGcc3NvTpJcCoNh+LcSLCx++Li0vFL99PnL19Xa2vqFyQrNoc0zmemrhBmQQkEbBUq4yjWwNJFwmdwcTfXLMWgjMnWOkxy6KRsqMRCcoad6taGNZ5fYUynGcKwBlKONf2QiC3A0HjG0E3dtY4NMO9e4vbaNqOma2/Qdxh1vbLperR62wlnR1yAqQZ2Uddar3cX9jBcpKOSSGdOJwhy7lmkUXIKrxoWBnPEbNoSOh4qlYLp2to2jPz3Tp4NM+6OQztj/OyxLjZmkiXemDEfmpTYl39I6/bHITTnrdj7s+SY4OOhaofICQfH5IoNCUszoNH3aFxo4yokHjGvh30L5iGnG0f9R1YcUvYzkNbjYaUV7r', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'd3fu/XDgzKuZbJFfpAGicg+OSQn5Iy0CSd/yD15II+Vu2Az+BZ8n1uDStmzQZ5V8OsJjRDCIQ==</latexit>(ˆy?(x(1)),ˆy?(x(2)))', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nSTA314-Lec1\\n\\n20 / 29\\n\\nBias/Variance Decomposition: Another Visualization\\n\\nThe Bayes error is an irreducible error that comes from the randomness in pdata(t\\n\\nx).\\n\\n|', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bayes optimal prediction\\n\\nvariance due to random test labels', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"A670NCeixZigUUibfD3V4OURW7k=\">AAACDnicbVDLTgIxFL3jE/GFunTTSExwQ2YIUZYkblxiIo8ERtLpFGjodCZthziZ8A+61f9wZ9z6C/6GX2CBWQh4kiYn59ybe3q8iDOlbfvb2tjc2t7Zze3l9w8Oj44LJ6ctFcaS0CYJeSg7HlaUM0GbmmlOO5GkOPA4bXvj25nfnlCpWCgedBJRN8BDwQaMYG2kdvKYlipX036haJftOdA6cTJShAyNfuGn54ckDqjQhGOluo4daTfFUjPC6TTfixWNMBnjIe0aKnBAlZvO407RpVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwbzpiRntZJ10qqUnety9b5arNeyunJwDhdQAgduoA530IAmEBjDC7zCm/VsvVsf1udidMPKds5gC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwy0nNE=</latexit>y(2)<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"PHUxT+Vwih3GhA51ADl0JcRjAgk=\">AAACDnicbVDLTgIxFL2DL8QX6tJNIzHBDZkxRFmSuHGJiTwSGEmnU6Ch05m0HeJkwj/oVv/DnXHrL/gbfoEFZiHgSZqcnHNv7unxIs6Utu1vK7exubW9k98t7O0fHB4Vj09aKowloU0S8lB2PKwoZ4I2NdOcdiJJceBx2vbGtzO/PaFSsVA86CSiboCHgg0YwdpI7eQxLTuX036xZFfsOdA6cTJSggyNfvGn54ckDqjQhGOluo4daTfFUjPC6bTQixWNMBnjIe0aKnBAlZvO407RhVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwYLpiRntZJ10rqqONeV6n21VK9ldeXhDM6hDA7cQB3uoAFNIDCGF3iFN+vZerc+rM/FaM7Kdk5hC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwsOnNA=</latexit>y(1)', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nSTA314-Lec1\\n\\n21 / 29\\n\\nBias/Variance Decomposition: Another Visualization\\n\\nSelecting a predictor ˆy (cid:63) variance.', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '∈ H\\n\\nfrom a training set comes with bias and\\n\\nBayes optimal prediction', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"A670NCeixZigUUibfD3V4OURW7k=\">AAACDnicbVDLTgIxFL3jE/GFunTTSExwQ2YIUZYkblxiIo8ERtLpFGjodCZthziZ8A+61f9wZ9z6C/6GX2CBWQh4kiYn59ybe3q8iDOlbfvb2tjc2t7Zze3l9w8Oj44LJ6ctFcaS0CYJeSg7HlaUM0GbmmlOO5GkOPA4bXvj25nfnlCpWCgedBJRN8BDwQaMYG2kdvKYlipX036haJftOdA6cTJShAyNfuGn54ckDqjQhGOluo4daTfFUjPC6TTfixWNMBnjIe0aKnBAlZvO407RpVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwbzpiRntZJ10qqUnety9b5arNeyunJwDhdQAgduoA530IAmEBjDC7zCm/VsvVsf1udidMPKds5gC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwy0nNE=</latexit>y(2)<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"PHUxT+Vwih3GhA51ADl0JcRjAgk=\">AAACDnicbVDLTgIxFL2DL8QX6tJNIzHBDZkxRFmSuHGJiTwSGEmnU6Ch05m0HeJkwj/oVv/DnXHrL/gbfoEFZiHgSZqcnHNv7unxIs6Utu1vK7exubW9k98t7O0fHB4Vj09aKowloU0S8lB2PKwoZ4I2NdOcdiJJceBx2vbGtzO/PaFSsVA86CSiboCHgg0YwdpI7eQxLTuX036xZFfsOdA6cTJSggyNfvGn54ckDqjQhGOluo4daTfFUjPC6bTQixWNMBnjIe0aKnBAlZvO407RhVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwYLpiRntZJ10rqqONeV6n21VK9ldeXhDM6hDA7cQB3uoAFNIDCGF3iFN+vZerc+rM/FaM7Kdk5hC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwsOnNA=</latexit>y(1) test labels', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'variance in predictionsdue to random training set\\n\\nresidual', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"IDYLCHeTqg3dS1Rh9s2o7msmjTE=\">AAACS3icbVDLSgMxFM1U66O+qi7dBItQN2VGirosiuBSwbZCZyyZNGODmQfJHWmJ8z/+jG7V39CduDBtB7TWC4GTc+7NPTl+IrgC236zCnPzxYXFpeXSyura+kZ5c6ul4lRS1qSxiOW1TxQTPGJN4CDYdSIZCX3B2v7d6Uhv3zOpeBxdwTBhXkhuIx5wSsBQ3fKJGxLo+74+yzraHb+nfZGyzO0T0MPsRrsKiMyy6qQx0INsHz/gn5vXLVfsmj0uPAucHFRQXhfd8rvbi2kasgioIEp1HDsBTxMJnAqWldxUsYTQO3LLOgZGJGTK02NvGd4zTA8HsTQnAjxmf09oEio1DH3TObKo/moj8j+t07vnicp3DSbLpp1AcOxpHiUpsIhOjASpwBDjUbC4xyWjIIYGECq5+QumfSIJBRN/yYTk/I1kFrQOas5hrX5ZrzSO87iW0A7aR', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'VXkoCPUQOfoAjURRY/oGb2gV+vJ+rA+ra9Ja8HKZ7bRVBWK38LVttA=</latexit>E[ˆy?(x)|x]', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'bias\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n22 / 29\\n\\nBias/Variance Decomposition: Another Visualization\\n\\nAn overly simple model (e.g. k-NN with large k) might have\\n\\n(cid:73) high bias (too simplistic to capture the structure in the data) (cid:73) low variance (there’s enough data to get a stable estimate of the', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'decision boundary)', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"IDYLCHeTqg3dS1Rh9s2o7msmjTE=\">AAACS3icbVDLSgMxFM1U66O+qi7dBItQN2VGirosiuBSwbZCZyyZNGODmQfJHWmJ8z/+jG7V39CduDBtB7TWC4GTc+7NPTl+IrgC236zCnPzxYXFpeXSyura+kZ5c6ul4lRS1qSxiOW1TxQTPGJN4CDYdSIZCX3B2v7d6Uhv3zOpeBxdwTBhXkhuIx5wSsBQ3fKJGxLo+74+yzraHb+nfZGyzO0T0MPsRrsKiMyy6qQx0INsHz/gn5vXLVfsmj0uPAucHFRQXhfd8rvbi2kasgioIEp1HDsBTxMJnAqWldxUsYTQO3LLOgZGJGTK02NvGd4zTA8HsTQnAjxmf09oEio1DH3TObKo/moj8j+t07vnicp3DSbLpp1AcOxpHiUpsIhOjASpwBDjUbC4xyWjIIYGECq5+QumfSIJBRN/yYTk/I1kFrQOas5hrX5ZrzSO87iW0A7aR', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'VXkoCPUQOfoAjURRY/oGb2gV+vJ+rA+ra9Ja8HKZ7bRVBWK38LVttA=</latexit>E[ˆy?(x)|x]', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"A670NCeixZigUUibfD3V4OURW7k=\">AAACDnicbVDLTgIxFL3jE/GFunTTSExwQ2YIUZYkblxiIo8ERtLpFGjodCZthziZ8A+61f9wZ9z6C/6GX2CBWQh4kiYn59ybe3q8iDOlbfvb2tjc2t7Zze3l9w8Oj44LJ6ctFcaS0CYJeSg7HlaUM0GbmmlOO5GkOPA4bXvj25nfnlCpWCgedBJRN8BDwQaMYG2kdvKYlipX036haJftOdA6cTJShAyNfuGn54ckDqjQhGOluo4daTfFUjPC6TTfixWNMBnjIe0aKnBAlZvO407RpVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwbzpiRntZJ10qqUnety9b5arNeyunJwDhdQAgduoA530IAmEBjDC7zCm/VsvVsf1udidMPKds5gC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwy0nNE=</latexit>y(2)<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"PHUxT+Vwih3GhA51ADl0JcRjAgk=\">AAACDnicbVDLTgIxFL2DL8QX6tJNIzHBDZkxRFmSuHGJiTwSGEmnU6Ch05m0HeJkwj/oVv/DnXHrL/gbfoEFZiHgSZqcnHNv7unxIs6Utu1vK7exubW9k98t7O0fHB4Vj09aKowloU0S8lB2PKwoZ4I2NdOcdiJJceBx2vbGtzO/PaFSsVA86CSiboCHgg0YwdpI7eQxLTuX036xZFfsOdA6cTJSggyNfvGn54ckDqjQhGOluo4daTfFUjPC6bTQixWNMBnjIe0aKnBAlZvO407RhVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwYLpiRntZJ10rqqONeV6n21VK9ldeXhDM6hDA7cQB3uoAFNIDCGF3iFN+vZerc+rM/FaM7Kdk5hC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwsOnNA=</latexit>y(1)Bayes optimal prediction', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nSTA314-Lec1\\n\\n23 / 29\\n\\nBias/Variance Decomposition: Another Visualization\\n\\nAn overly complex model (e.g. KNN with k = 1) may have\\n\\n(cid:73) low bias (since it learns all the relevant structure) (cid:73) high variance (it ﬁts the quirks of the data you happened to sample)', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Bayes optimal prediction', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"IDYLCHeTqg3dS1Rh9s2o7msmjTE=\">AAACS3icbVDLSgMxFM1U66O+qi7dBItQN2VGirosiuBSwbZCZyyZNGODmQfJHWmJ8z/+jG7V39CduDBtB7TWC4GTc+7NPTl+IrgC236zCnPzxYXFpeXSyura+kZ5c6ul4lRS1qSxiOW1TxQTPGJN4CDYdSIZCX3B2v7d6Uhv3zOpeBxdwTBhXkhuIx5wSsBQ3fKJGxLo+74+yzraHb+nfZGyzO0T0MPsRrsKiMyy6qQx0INsHz/gn5vXLVfsmj0uPAucHFRQXhfd8rvbi2kasgioIEp1HDsBTxMJnAqWldxUsYTQO3LLOgZGJGTK02NvGd4zTA8HsTQnAjxmf09oEio1DH3TObKo/moj8j+t07vnicp3DSbLpp1AcOxpHiUpsIhOjASpwBDjUbC4xyWjIIYGECq5+QumfSIJBRN/yYTk/I1kFrQOas5hrX5ZrzSO87iW0A7aR', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'VXkoCPUQOfoAjURRY/oGb2gV+vJ+rA+ra9Ja8HKZ7bRVBWK38LVttA=</latexit>E[ˆy?(x)|x]', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"A670NCeixZigUUibfD3V4OURW7k=\">AAACDnicbVDLTgIxFL3jE/GFunTTSExwQ2YIUZYkblxiIo8ERtLpFGjodCZthziZ8A+61f9wZ9z6C/6GX2CBWQh4kiYn59ybe3q8iDOlbfvb2tjc2t7Zze3l9w8Oj44LJ6ctFcaS0CYJeSg7HlaUM0GbmmlOO5GkOPA4bXvj25nfnlCpWCgedBJRN8BDwQaMYG2kdvKYlipX036haJftOdA6cTJShAyNfuGn54ckDqjQhGOluo4daTfFUjPC6TTfixWNMBnjIe0aKnBAlZvO407RpVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwbzpiRntZJ10qqUnety9b5arNeyunJwDhdQAgduoA530IAmEBjDC7zCm/VsvVsf1udidMPKds5gC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwy0nNE=</latexit>y(2)<latexit', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'sha1_base64=\"PHUxT+Vwih3GhA51ADl0JcRjAgk=\">AAACDnicbVDLTgIxFL2DL8QX6tJNIzHBDZkxRFmSuHGJiTwSGEmnU6Ch05m0HeJkwj/oVv/DnXHrL/gbfoEFZiHgSZqcnHNv7unxIs6Utu1vK7exubW9k98t7O0fHB4Vj09aKowloU0S8lB2PKwoZ4I2NdOcdiJJceBx2vbGtzO/PaFSsVA86CSiboCHgg0YwdpI7eQxLTuX036xZFfsOdA6cTJSggyNfvGn54ckDqjQhGOluo4daTfFUjPC6bTQixWNMBnjIe0aKnBAlZvO407RhVF8NAileUKjufp3I8WBUkngmckA65Fa9Wbif17Xn7BIZbeeFseWk+hBzU2ZiGJNBVkEGcQc6RDNukE+k5RonhiCiWTmL4iMsMREmwYLpiRntZJ10rqqONeV6n21VK9ldeXhDM6hDA7cQB3uoAFNIDCGF3iFN+vZerc+rM/FaM7Kdk5hC', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'dbXLwsOnNA=</latexit>y(1)', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Intro ML (UofT)\\n\\nSTA314-Lec1\\n\\n24 / 29\\n\\nValidation\\n\\nBefore we move on to bagging, it’s a good time to mention validation.\\n\\nWe may want to assess how likely a learning algorithm is to generalize before picking one and reporting the ﬁnal test error.\\n\\nIn other words, until now we’ve been picking predictors that optimize the training loss, but we want a technique for picking predictors that are likely to generalize as well.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n25 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Validation\\n\\nFor example, we may want to assess the following types of choices:\\n\\n1. Hyper-parameters of the learning algorithm that lead to better generalization. Often there are parameters that cannot be ﬁt on the training set, e.g., k in k-NN, because the training set would give meaningless answers about the best setting, i.e., k = 1 is always gives optimal training set loss for k-NN.', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '2. Picking predictors that generalize better. E.g., should we use a decision tree or k-NN if we want to generalize?\\n\\nWe make these choices using validation to avoid measuring test loss (then the test set would no longer be unseen data!).\\n\\nSuppose we are trying to estimate the generalization of two learning algorithms, e.g., a decision tree and a k-NN model.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n26 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Hold-out validation\\n\\nOriginal Training Set\\n\\nValidation\\n\\nTypes of Validation Hold-Out Validation: Split data into training and validation sets. •Usually 30% as hold-out set. Problems: •Waste of dataset •Estimation of error rate might be misleading\\n\\nThe most common method of validation is to hold-out a subset of the training set and use it to assess how likely we are to generalize to unseen data.', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'In our example of deciding between a decision tree and k-NN in terms of generalization, we would ﬁt ˆy (cid:63) the average loss on the validation set', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'kNN and ˆy (cid:63)\\n\\nd-tree on the training set and measure\\n\\nˆ R\\n\\nWe pick the predictor ˆy (cid:63)\\n\\nvalid ] vs. ˆ R', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'valid ]\\n\\n[ˆy (cid:63)\\n\\n[ˆy (cid:63)\\n\\nkNN,\\n\\nd-tree,\\n\\nD kNN vs. ˆy (cid:63)\\n\\nD\\n\\nd-tree with lowest validation loss.\\n\\nProblem: this is usually a waste of data.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n27 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'K -fold cross validation\\n\\nSecond most common way: partition training data randomly into K equally sized subsets. For each “turn”, use the ﬁrst K training data and the last subset as validation\\n\\n1 subsets (or “folds”) as\\n\\n−\\n\\nVariants of Cross-Validation K-fold: Partition training data into K equally sized subsamples. For each fold, use the other K-1 subsamples as training data with the last subsample as validation.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n28 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'K -fold cross validation\\n\\nVariants of Cross-Validation K-fold: Partition training data into K equally sized subsamples. For each fold, use the other K-1 subsamples as training data with the last subsample as validation.\\n\\nIn our running example: ﬁt a new predictor using each learning algorithm on K 1 folds for each of the K turns, and measure the validation loss on the held-out fold, averaged over the turns:\\n\\n−', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'K\\n\\nK\\n\\n1 K\\n\\n1 K\\n\\nˆ R\\n\\nˆ R\\n\\nvalid i D\\n\\nvalid i D\\n\\n[ˆy (cid:63)\\n\\n[ˆy (cid:63)\\n\\n]\\n\\n] vs.\\n\\nkNN,i ,\\n\\nd−tree,i ,\\n\\ni=1 (cid:88)\\n\\ni=1 (cid:88)\\n\\nwhere ˆy (cid:63) algorithm A and\\n\\nA,i is the predictor ﬁt on the training subset of the ith turn using', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'valid i D\\n\\nis the validation subset of the ith turn.\\n\\nWe pick the learning algorithm, e.g., k-NN v. decision tree, with lowest validation loss averaged across the K turns.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n29 / 29', 'filename': 'data/coursematerial/test_data/lec03.pdf'}\n",
      "Uploading chunk\n",
      "Uploading 102 chunks for data/coursematerial/test_data/lec02.pdf.\n",
      "{'content': 'STA 314: Statistical Methods for Machine Learning I Lecture 2 - Decision Trees\\n\\nChris J. Maddison\\n\\nUniversity of Toronto\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n1 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ElthYt0itatHt\\n\\narg min & arg max\\n\\nGiven a function f : Rd its minimum point, i.e., the point x (cid:63) such that for all x\\n\\nR, we may want Rd\\n\\n→\\n\\n∈', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Rd\\n\\n∈ f (x (cid:63))\\n\\nf (x)\\n\\n≤\\n\\narg min returns the minimum point,\\n\\nx (cid:63) = arg min x∈Rd\\n\\nf (x)\\n\\narg max returns the maximum point.\\n\\na)2 = a.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'arg minx∈R (x\\n\\n−\\n\\nIf there is more than one minimum or maximum point, then the arg min or arg max are sets.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n2 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Today\\n\\nDecision Trees\\n\\n(cid:73) Simple but powerful learning algorithm (cid:73) Used widely in Kaggle competitions (cid:73) Lets us motivate concepts from information theory (entropy, mutual', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'information, etc.)\\n\\nLoss functions and the question of generalization\\n\\n(cid:73) We’ve been dancing around this question, let’s formalize it a bit.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n3 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Trees\\n\\nMake predictions by splitting on attributes according to a tree structure.\\n\\nYes No Yes No', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Yes No\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n4 / 60\\n\\nDecision Trees\\n\\nMake predictions by splitting on attributes according to a tree structure.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n5 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Trees—Discrete attributes\\n\\nFirst, what if attributes are discrete?\\n\\nattributes:\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n6 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Trees—Discrete attributes\\n\\nSplit discrete attributes into a partition of possible values.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n7 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Trees—Continuous attributes\\n\\nFor continuous attributes, we partition the range by checking whether that attribute is greater than or less than some threshold.\\n\\nDecision boundary is made up of axis-aligned planes.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n8 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Trees\\n\\nYes No Yes No\\n\\nYes No\\n\\nInternal nodes test a attribute, i.e., a dimension of the representation.\\n\\nBranching is determined by the attribute value.\\n\\nChildren of a node partition the range of the attribute from the parent.\\n\\nLeaf nodes are outputs (predictions).\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n9 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Trees—Classiﬁcation and Regression\\n\\nEach path from root to a leaf deﬁnes a region Rm of input space\\n\\n(x (m1), t (m1)), . . . , (x (mk ), t (mk )) Let { training examples that fall into Rm\\n\\n}\\n\\nbe the\\n\\nClassiﬁcation tree (we will focus on this):\\n\\n(cid:73) discrete output (cid:73) leaf value y m typically set to the most common value in\\n\\nt (m1), . . . , t (mk ) {\\n\\n}', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Regression tree:\\n\\n(cid:73) continuous output (cid:73) leaf value y m typically set to the mean value in\\n\\nt (m1), . . . , t (mk ) {\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n}\\n\\n10 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Learning Decision Trees\\n\\nFor any training set we can construct a decision tree that has exactly the one leaf for every training point, but it probably won’t generalize.\\n\\n(cid:73) Decision trees are universal function approximators.\\n\\nBut, ﬁnding the smallest decision tree that correctly classiﬁes a training set is computationally challenging.\\n\\n(cid:73) If you are interested, check: Hyaﬁl & Rivest’76.\\n\\nSo, how do we construct a useful decision tree?\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n11 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Learning Decision Trees\\n\\nResort to a greedy heuristic:\\n\\n(cid:73) Start with the whole training set and an empty decision tree. (cid:73) Pick a attribute and candidate split that would most reduce the loss. (cid:73) Split on that attribute and recurse on subpartitions.\\n\\nWhich loss should we use?\\n\\n(cid:73) Let’s see if misclassiﬁcation rate is a good loss.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n12 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Choosing a Good Split\\n\\nConsider the following data. Let’s split on width.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n13 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Choosing a Good Split\\n\\nRecall: classify by majority.\\n\\nA and B have the same misclassiﬁcation rate, so which is the best split? Vote!\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n14 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Choosing a Good Split\\n\\nA feels like a better split, because the left-hand region is very certain about whether the fruit is an orange.\\n\\nCan we quantify this?\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n15 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Choosing a Good Split\\n\\nHow can we quantify uncertainty in prediction for a given leaf node?\\n\\n(cid:73) If all examples in leaf have same class: good, low uncertainty (cid:73) If each class has same amount of examples in leaf: bad, high', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'uncertainty\\n\\nIdea: Use counts at leaves to deﬁne probability distributions; use a probabilistic notion of uncertainty to decide splits.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'A brief detour through information theory...\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n16 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Quantifying Uncertainty\\n\\nThe entropy of a discrete random variable is a number that quantiﬁes the uncertainty inherent in its possible outcomes.\\n\\nThe mathematical deﬁnition of entropy that we give in a few slides may seem arbitrary, but it can be motivated axiomatically.\\n\\n(cid:73) If you’re interested, check: Information Theory by Robert Ash.\\n\\nTo explain entropy, consider ﬂipping two diﬀerent coins...\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n17 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'We Flip Two Diﬀerent Coins\\n\\nSequence 1: 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ... ?\\tSequence 2: 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 ... ?\\n\\n16 2\\n\\n8 10 0\\t1\\tversus 0\\t1\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n18 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Quantifying Uncertainty\\n\\nThe entropy of a loaded coin with probability p of heads is given by\\n\\n−\\n\\np log2(p)\\n\\n−\\n\\n(1\\n\\n−', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'p) log2(1\\n\\n−\\n\\np)\\n\\n0\\t1\\t8/9 1/9\\n\\n0\\t1\\t4/9 5/9\\n\\n−\\n\\n8 9\\n\\nlog2\\n\\n8 9 −\\n\\n1 9\\n\\nlog2\\n\\n1 9 ≈\\n\\n1 2\\n\\n−\\n\\n4 9\\n\\nlog2\\n\\n4 9 −\\n\\n5 9', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'log2\\n\\n5 9 ≈\\n\\n0.99\\n\\nNotice: the coin whose outcomes are more certain has a lower entropy.\\n\\nIn the extreme case p = 0 or p = 1, we were certain of the outcome before observing. So, we gained no certainty by observing it, i.e., entropy is 0.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n19 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Quantifying Uncertainty\\n\\n0.8\\n\\n0.2\\n\\n0.2\\n\\nentropy\\n\\nprobabilitypofheads\\n\\n0.6\\n\\n0.6\\n\\n0.8\\n\\n1.0\\n\\n0.4\\n\\n0.4\\n\\n1.0\\n\\nClaude Shannon showed: you cannot store the outcome of a random draw using fewer expected bits than the entropy without losing information.\\n\\nSo units of entropy are bits; a fair coin ﬂip has 1 bit of entropy.\\n\\n(cid:73) So, entropy can be seen as the expected information content of a', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'random variable.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n20 / 60\\n\\nEntropy\\n\\nMore generally, the entropy of a discrete random variable Y is given by\\n\\nH(Y ) =\\n\\n−\\n\\n(cid:88)y ∈Y', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'p(y ) log2 p(y )\\n\\nInterpret p(y ) log2 p(y ) = 0 if p(y ) = 0. “High Entropy”:\\n\\n(cid:73) Variable has a uniform like distribution over many outcomes (cid:73) Flat histogram (cid:73) Values sampled from it are less predictable', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '“Low Entropy”\\n\\n(cid:73) Distribution is concentrated on only a few outcomes (cid:73) Histogram is concentrated in a few areas (cid:73) Values sampled from it are more predictable\\n\\nIntro ML (UofT) [Slide credit: Vibhav Gogate]\\n\\nSTA314-Lec1\\n\\n21 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Entropy\\n\\nSuppose we observe partial information X about a random variable Y\\n\\n(cid:73) For example, X = sign(Y ).\\n\\nWe want to work towards a deﬁnition of the expected amount of information that will be conveyed about Y by observing X .\\n\\n(cid:73) Or equivalently, the expected reduction in our uncertainty about Y\\n\\nafter observing X .\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n22 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': \"Entropy of a Joint Distribution\\n\\nExample: X = Y =\\n\\nRaining, Not raining { Cloudy, Not cloudy {\\n\\n}\\n\\n}\\n\\n,\\n\\nCloudy'Not'Cloudy'Raining'24/100'1/100'Not'Raining'25/100'50/100'\\n\\nH(X , Y ) =\\n\\n=\\n\\n−\\n\\n−\\n\\n(cid:88)x∈X (cid:88)y ∈Y 24 log2 100\\n\\np(x, y ) log2 p(x, y )\\n\\n24 100 −\\n\\n1 100\", 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'log2\\n\\n1 100 −\\n\\n25 100\\n\\nlog2\\n\\n25 100 −\\n\\n50 100\\n\\nlog2\\n\\n≈\\n\\n1.56bits\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n50 100\\n\\n23 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': \"Speciﬁc Conditional Entropy\\n\\nExample: X = Y =\\n\\nRaining, Not raining { Cloudy, Not cloudy {\\n\\n}\\n\\n}\\n\\n,\\n\\nCloudy'Not'Cloudy'Raining'24/100'1/100'Not'Raining'25/100'50/100'\\n\\nWhat is the entropy of cloudiness Y , given that it is raining?\", 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H(Y\\n\\nX = x) = |\\n\\n=\\n\\n−\\n\\n−\\n\\np(y\\n\\n(cid:88)y ∈Y 24 25\\n\\nlog2\\n\\nx) log2 p(y |\\n\\n|\\n\\nx)\\n\\n24 25 −\\n\\n1 25\\n\\nlog2\\n\\n1 25 ≈', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '0.24bits\\n\\nWe used: p(y\\n\\nx) = p(x,y ) p(x) , |\\n\\nand p(x) =\\n\\ny p(x, y )\\n\\n(sum in a row)\\n\\n(cid:80)\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n24 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Conditional Entropy\\n\\nExample: X = Y =\\n\\nRaining, Not raining { Cloudy, Not cloudy {\\n\\n}\\n\\n}\\n\\n,\\n\\nThe expected conditional entropy:', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': \"H(Y\\n\\n|\\n\\nX ) =\\n\\n=\\n\\nIntro ML (UofT)\\n\\nCloudy'Not'Cloudy'Raining'24/100'1/100'Not'Raining'25/100'50/100'\\n\\n(cid:88)x∈X\", 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'p(x)H(Y\\n\\n|\\n\\nX = x)\\n\\n−\\n\\n(cid:88)x∈X (cid:88)y ∈Y\\n\\np(x, y ) log2 p(y\\n\\nx) |\\n\\nSTA314-Lec1\\n\\n25 / 60\\n\\nConditional Entropy', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': \"Example: X = Y =\\n\\nRaining, Not raining { Cloudy, Not cloudy {\\n\\n}\\n\\n}\\n\\n,\\n\\nCloudy'Not'Cloudy'Raining'24/100'1/100'Not'Raining'25/100'50/100'\\n\\nEntropy of cloudiness given the knowledge of whether or not it is raining?\", 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H(Y\\n\\nX ) = |\\n\\n=\\n\\np(x)H(Y\\n\\nX = x) |\\n\\n(cid:88)x∈X 1 4 0.75 bits\\n\\n3 4\\n\\nH(Y\\n\\nis raining) + |\\n\\nH(Y\\n\\nnot raining) |\\n\\n≈\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n26 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Conditional Entropy\\n\\nSome useful properties:\\n\\n(cid:73) Non-negative: H(X ) (cid:73) Chain rule: H(X , Y ) = H(X (cid:73) Independence: If X and Y independent, then X does not aﬀect our\\n\\n0\\n\\n≥\\n\\nY ) + H(Y ) = H(Y\\n\\nX ) + H(X ) |\\n\\n|', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'uncertainty about Y : H(Y\\n\\nX ) = H(Y ) |\\n\\n(cid:73) Knowing Y makes our knowledge of Y certain: H(Y (cid:73) Knowing X can only decrease uncertainty about Y : H(Y\\n\\nY ) = 0 | X ) |\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n≤\\n\\nH(Y )\\n\\n27 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': \"Information Gain\\n\\nCloudy'Not'Cloudy'Raining'24/100'1/100'Not'Raining'25/100'50/100'\\n\\nHow much more certain am I about whether it’s cloudy if I’m told whether it is raining?\\n\\n(cid:73) My uncertainty in Y minus my expected uncertainty that would remain\\n\\nin Y after seeing X .\\n\\nThis is the information gain IG (Y , X ) in Y due to X , or the mutual information of Y and X\\n\\nIG (Y , X ) = H(Y )\\n\\n−\", 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H(Y\\n\\nX ) |\\n\\nIf X is completely uninformative about Y : IG (Y , X ) = 0\\n\\nIf X is completely informative about Y : IG (Y , X ) = H(Y )\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n(1)\\n\\n28 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Revisiting Our Original Example\\n\\nInformation gain measures the informativeness of a variable, which is exactly what we desire in a decision tree split!\\n\\nThe information gain of a split: how much information (over the training set) about the class label, Y = , is gained by knowing that you red, blue { are considering data on one side of the split, X =\\n\\n}', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'left, right {\\n\\n.\\n\\n}\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n29 / 60\\n\\nRevisiting Our Original Example\\n\\nLet’s compute IG (Y , X ) for example.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n30 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Revisiting Our Original Example\\n\\nWhat is the information gain of split B? Not terribly informative...\\n\\n7 log2( 5 7 ) ≈ 0.81,\\n\\n2\\n\\n7 log2( 2 7 ) − X = left)\\n\\n5', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Root entropy of class outcome: H(Y ) =\\n\\n−\\n\\nLeaf conditional entropy of class outcome: H(Y H(Y\\n\\n≈\\n\\n|\\n\\nX = right)\\n\\n0.92\\n\\n| IG (Y , X )\\n\\n≈\\n\\n( 4 7 ·\\n\\n0.81 + 3\\n\\n0.92)\\n\\n0.006\\n\\n0.86\\n\\n7 ·\\n\\n≈\\n\\n−\\n\\n≈\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n0.86\\n\\n31 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Revisiting Our Original Example\\n\\nWhat is the information gain of split A? Very informative!\\n\\n5\\n\\n2\\n\\n7 log2( 5 7 log2( 2 7 ) 7 ) X = left) = 0,', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Root entropy of class outcome: H(Y ) =\\n\\n−\\n\\n−\\n\\nLeaf conditional entropy of class outcome: H(Y H(Y\\n\\n|\\n\\n0.97', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'X = right)\\n\\n≈\\n\\n| IG (Y , X )\\n\\n0 + 5\\n\\n( 2 7 ·\\n\\n0.97)\\n\\n0.86\\n\\n0.17!!\\n\\n7 ·\\n\\n≈\\n\\n−\\n\\n≈\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n≈\\n\\n0.86\\n\\n32 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Constructing Decision Trees\\n\\nYes No Yes No\\n\\nYes No\\n\\nAt each level, one must choose:\\n\\n1. Which attribute to split. 2. Possibly where to split it.\\n\\nChoose them based on how much information we would gain from the decision! (choose attribute that gives the highest gain)\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n33 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Tree Construction Algorithm\\n\\nSimple, greedy, recursive approach, builds up tree node-by-node\\n\\n1. pick a attribute to split at a non-terminal node 2. split examples into groups based on attribute value 3. for each group: (cid:73) if no examples – return majority from parent (cid:73) else if all examples in same class – return class (cid:73) else loop to step 1\\n\\nTerminates when all leaves contain only examples in the same class or are empty.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n34 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Back to Our Example\\n\\nattributes:\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n[from: Russell & Norvig]\\n\\n35 / 60\\n\\nattribute Selection', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'IG (Type, Y ) = 1\\n\\n−\\n\\n= 0\\n\\nIG (Patron, Y ) = 1\\n\\n−\\n\\nIntro ML (UofT)\\n\\n(cid:20) 2 12\\n\\n(cid:20) 2 12\\n\\nH(Y\\n\\n|\\n\\nFr.) +\\n\\n2 12', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H(Y\\n\\n|\\n\\nIt.) +\\n\\nH(0, 1) +\\n\\n4 12\\n\\nH(1, 0) +\\n\\n6 12\\n\\nSTA314-Lec1\\n\\n4 12\\n\\nH(\\n\\nH(Y\\n\\n|\\n\\nThai) +\\n\\n2 6\\n\\n,\\n\\n4 6\\n\\n)\\n\\n(cid:21)\\n\\n≈\\n\\n0.541\\n\\n4 12', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H(Y\\n\\n|\\n\\nBur.)\\n\\n(cid:21)\\n\\n36 / 60\\n\\nWhich Tree is Better? Vote!\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n37 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'What Makes a Good Tree?\\n\\nNot too small: need to handle important but possibly subtle distinctions in data', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Not too big:\\n\\n(cid:73) Computational eﬃciency (avoid redundant, spurious attributes) (cid:73) Avoid over-ﬁtting training examples (cid:73) Human interpretability\\n\\n“Occam’s Razor”: ﬁnd the simplest hypothesis that ﬁts the observations\\n\\n(cid:73) Useful principle, but hard to formalize (how to deﬁne simplicity?) (cid:73) See Domingos, 1999, “The role of Occam’s razor in knowledge', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'discovery”\\n\\nWe desire small trees with informative nodes near the root\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n38 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Decision Tree Miscellany', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Problems:\\n\\n(cid:73) You have exponentially less data at lower levels (cid:73) Too big of a tree can overﬁt the data (cid:73) Greedy algorithms don’t necessarily yield the global optimum\\n\\nHandling continuous attributes\\n\\n(cid:73) Split based on a threshold, chosen to maximize information gain\\n\\nDecision trees can also be used for regression on real-valued outputs. Choose splits to minimize squared error, rather than maximize information gain.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n39 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Comparison to k-NN\\n\\nAdvantages of decision trees over k-NN\\n\\nGood when there are lots of attributes, but only a few are important', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Good with discrete attributes\\n\\nEasily deals with missing values (just treat as another value)\\n\\nRobust to scale of inputs', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Fast at test time\\n\\nMore interpretable\\n\\nAdvantages of k-NN over decision trees\\n\\nFew hyperparameters\\n\\nAble to handle attributes/features that interact in complex ways (e.g. pixels)\\n\\nCan incorporate interesting distance measures (e.g. shape contexts)\\n\\nTypically make better predictions in practice Intro ML (UofT)\\n\\nSTA314-Lec1\\n\\n40 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Today, we deepen our understanding of generalization.\\n\\n(cid:73) This will help us understand how to combine classiﬁers to get better\\n\\nperformance (ensembling methods).\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n41 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Learning & Generalization\\n\\nRecall that we said that overly simple learning algorithms underﬁt the data, and overly complex ones overﬁt.\\n\\nToday we will be a bit more precise about what this means and what the goal of supervised learning is in general.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n42 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Loss Functions\\n\\nGiven an input-label pair (x, t), a loss function L(y , t) deﬁnes how bad it is if the algorithm predicts y .', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Example: 0-1 loss for classiﬁcation\\n\\nL0−1(y , t) =\\n\\n0 1 (cid:40)\\n\\nif y = t if y = t\\n\\n(cid:54)\\n\\n(cid:73) Average 0-1 loss gives the error rate.\\n\\nExample: squared error loss for regression', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'LSE(y , t) =\\n\\n1 2\\n\\n(y\\n\\n−\\n\\nt)2\\n\\n(cid:73) The average squared error loss is called mean squared error (MSE).\\n\\nLet’s focus on 0-1 loss with inputs x\\n\\n∈\\n\\nRd and labels t\\n\\n∈ {\\n\\n0, 1\\n\\n. }\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n43 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Loss Functions\\n\\nBoth k-NN and decision trees make predictions for all queries x.\\n\\nWe can think of the predictions of our learning algorithm forming a mapping y : Rd For a random data point drawn (x, t) distribution, we can measure the expected error for the predictor y :\\n\\n0, 1 }\\n\\nthat we call a predictor.\\n\\n→ {\\n\\npdata from some data generating\\n\\n∼', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'R\\n\\n[y ] :=\\n\\n(cid:88)t∈{0,1} (cid:90)\\n\\nL0−1(y (x), t)pdata(x, t) dx\\n\\nFor a ﬁnite data set\\n\\nD\\n\\nˆ R\\n\\n=\\n\\n[y ,\\n\\n(x(i), t (i)) {\\n\\nN i=1, we can measure the average error: } N', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '1 N\\n\\nL0−1(y (x(i)), t (i))\\n\\n] :=\\n\\nD\\n\\n(cid:88)i=1\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n44 / 60\\n\\nGoal of Supervised Learning\\n\\nFind a predictor y that achieves the lowest expected loss.\\n\\ny ∗ = arg min\\n\\ny :Rd →{0,1} R\\n\\n[y ]\\n\\n(cid:73) If we’re performing regression, we will optimize over y : Rd (cid:73) If we’re performing classiﬁcation, we will optimize over', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'y : Rd\\n\\n→ {\\n\\n1, . . . , C\\n\\n}\\n\\n.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n→\\n\\nR.\\n\\n45 / 60\\n\\nElthYt0itatHt\\n\\nExample', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'uniform[0, 1]\\n\\nx\\n\\n∼\\n\\nif x < 0.5 if x 0.5\\n\\n0 1\\n\\nt(x) =\\n\\n(cid:40)\\n\\n≥\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n46 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'InistakeYttit0,0\\n\\nExample\\n\\nuniform[0, 1]\\n\\nx\\n\\n∼\\n\\nif x < 0.5 if x 0.5\\n\\n0 1\\n\\nt(x) =\\n\\n(cid:40)\\n\\n≥\\n\\nWhat is the expected error?\\n\\nif x < 0.75 if x 0.75\\n\\n0 1 (cid:40)\\n\\ny (x) =\\n\\n(2)\\n\\n≥\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n47 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ElthYt0itatHt\\n\\nExample\\n\\nuniform[0, 1]\\n\\nx\\n\\n∼\\n\\nif x < 0.5 if x 0.5\\n\\n0 1 y (cid:63)(x) = t(x)\\n\\nt(x) =\\n\\n(cid:40)\\n\\n≥\\n\\nOpt. predictor is y (cid:63) = t.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n48 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Supervised Learning in practice\\n\\ny is taken from a more restricted set of functions called a hypothesis space.\\n\\nH ⊂ {', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'y : Rd\\n\\n→ {\\n\\n0, 1\\n\\n(cid:73)\\n\\nmay correspond to the set of all decisions boundaries that can be\\n\\nH representation by a k-NN algorithm.\\n\\n(cid:73)\\n\\nmay correspond to the set of all decisions boundaries that can be', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H representation by a decision tree.\\n\\n(x(i), t (i))\\n\\nN i=1, which we assume to be }\\n\\nWe have a training set { independent and identically distributed (i.i.d.) draws from pdata.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Dtrain =\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n}}\\n\\n49 / 60\\n\\nSupervised Learning in practice\\n\\nPick y by minimizing the loss on the training set', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'min y ∈H\\n\\nˆ R\\n\\n[y ,\\n\\nDtrain]\\n\\n→\\n\\nˆy (cid:63)\\n\\nBut we really care about performance of ˆy (cid:63) in terms of expected loss.\\n\\nSo, we measure its average error on an unseen test set Dtest = true data generating distribution,\\n\\n(x(i), t (i))\\n\\nM i=1 i.i.d. pdata to approximate how well it does on the }\\n\\n{', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ˆ R\\n\\n[ˆy (cid:63),\\n\\n[ˆy (cid:63)]\\n\\nDtest] We say that we want ˆy (cid:63) to generalize from the training set to the test set.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '≈ R\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n50 / 60\\n\\nUnderﬁtting & Overﬁtting\\n\\nThis is the essence of supervised learning.\\n\\n(cid:73) many open questions, depending on the choice of (cid:73) can study this problem as N\\n\\n.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'H changes.\\n\\nor as\\n\\nH changes and return to underﬁtting and overﬁtting.\\n\\n→ ∞\\n\\nLet’s study this as\\n\\nH\\n\\nunderﬁtting', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Typically,wewillseeinlaterchaptersthattheestimationerrorisoftendecomposedas,forθ!aminimizeronΘoftheexpectedriskR(fθ!):!R(fˆθ)−R(fθ!)\"=!R(fˆθ)−ˆR(fˆθ)\"+!ˆR(fˆθ)−ˆR(fθ!)\"+!ˆR(fθ!)−R(fθ!)\"!2supθ∈Θ###ˆR(fθ)−R(fθ)###+empiricaloptimizationerror.Theuniformdeviationgrowswiththe“size”ofΘ,andusuallydecayswithn.SeemoredetailsinChapter4.Capacitycontrol.Inordertoavoidoverﬁtting,weneedtomakesurethatthesetofallowedfunctionsisnottoolarge,bytypicallyreducingthenumberofparameters,orbyrestrictingthenormofpredictor', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 's(thusbyreducingthe“size”ofΘ):thistypicallyleadstoconstrainedoptimization,andallowsforriskdecompositionsasdoneabove.Capacitycontrolcanalsobedonebyregularization,thatis,byminimizingˆR(fθ)+λΩ(θ)=1 47/50', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '“size”ofΘErrorstesttrainoverﬁtting\\n\\n26CHAPTER2.INTRODUCTIONTOSUPERVISEDLEARNING\\n\\nSource: Francis Bach. Learning Theory from First Principles.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n51 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ifte0,0XEOI1,1Dte0,0XEOI1,1ite0,0\\n\\n2D Example\\n\\nx is uniform on the ellipses.\\n\\nt depends on which ellipse x falls in\\n\\n, ∈ {•\\n\\n}\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n52 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'ifte0,0XEOI1,1Dte0,0XEOI1,1ite0,0\\n\\nifte0,0XEOI1,1Dte0,0XEOI1,1ite0,0\\n\\n2D Example\\n\\nTrain Set\\n\\nTest Set\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n53 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'mistakesmistakes0,00,0\\n\\n2D Example\\n\\nLet’s consider a simple hypothesis class.\\n\\ny with vertical decision H { . boundaries }\\n\\n=\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n54 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'mistakesmistakes0,00,0\\n\\nmistakesmistakes0,00,0\\n\\n2D Example\\n\\nBest predictor on training set does poorly on both the training set and test set.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Train Set\\n\\nTest Set\\n\\nThis is underﬁtting.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n55 / 60\\n\\n4DI0,0iIgD0,0\\n\\n2D Example\\n\\nLet’s consider a more complex hypothesis class.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'y with linear decision H { . boundaries }\\n\\n=\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n56 / 60\\n\\n4DI0,0iIgD0,0\\n\\n4DI0,0iIgD0,0\\n\\n2D Example\\n\\nBest predictor on training set does well on both the training set and test set.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Train Set\\n\\nTest Set\\n\\nThis is well ﬁt.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n57 / 60\\n\\ncan0,0\\n\\n2D Example\\n\\nLet’s consider a very complex hypothesis class.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'y with curved decision H { . boundaries }\\n\\n=\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n58 / 60\\n\\ncan0,0\\n\\ncan0,0\\n\\n2D Example\\n\\nBest predictor on training set does poorly on test set, but well on training set.', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Train Set\\n\\nTest Set\\n\\nThis is overﬁtting.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n59 / 60\\n\\nSummary\\n\\nWe have now talked about two hypothesis classes: k-NN and decision trees.\\n\\nWe can understand supervised learning through the complexity of the hypothesis class.\\n\\nunderﬁtting', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 'Typically,wewillseeinlaterchaptersthattheestimationerrorisoftendecomposedas,forθ!aminimizeronΘoftheexpectedriskR(fθ!):!R(fˆθ)−R(fθ!)\"=!R(fˆθ)−ˆR(fˆθ)\"+!ˆR(fˆθ)−ˆR(fθ!)\"+!ˆR(fθ!)−R(fθ!)\"!2supθ∈Θ###ˆR(fθ)−R(fθ)###+empiricaloptimizationerror.Theuniformdeviationgrowswiththe“size”ofΘ,andusuallydecayswithn.SeemoredetailsinChapter4.Capacitycontrol.Inordertoavoidoverﬁtting,weneedtomakesurethatthesetofallowedfunctionsisnottoolarge,bytypicallyreducingthenumberofparameters,orbyrestrictingthenormofpredictor', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': 's(thusbyreducingthe“size”ofΘ):thistypicallyleadstoconstrainedoptimization,andallowsforriskdecompositionsasdoneabove.Capacitycontrolcanalsobedonebyregularization,thatis,byminimizingˆR(fθ)+λΩ(θ)=1 47/50', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n",
      "{'content': '“size”ofΘErrorstesttrainoverﬁtting\\n\\n26CHAPTER2.INTRODUCTIONTOSUPERVISEDLEARNING\\n\\nSource: Francis Bach. Learning Theory from First Principles.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n60 / 60', 'filename': 'data/coursematerial/test_data/lec02.pdf'}\n",
      "Uploading chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 80.58%, threshold set to 80.00%\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate\",\"time\":\"2024-03-30T15:56:01-04:00\"}\n"
     ]
    }
   ],
   "source": [
    "from weaviate import Client\n",
    "import time\n",
    "\n",
    "\n",
    "def configure_batch(client: Client, batch_size: int, batch_target_rate: int):\n",
    "    \"\"\"\n",
    "    Configure the weaviate client's batch so it creates objects at `batch_target_rate`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    client : Client\n",
    "        The Weaviate client instance.\n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "    batch_target_rate : int\n",
    "        The batch target rate as # of objects per second.\n",
    "    \"\"\"\n",
    "\n",
    "    def callback(batch_results: dict) -> None:\n",
    "\n",
    "        # you could print batch errors here\n",
    "        time_took_to_create_batch = batch_size * (client.batch.creation_time/client.batch.recommended_num_objects)\n",
    "        time.sleep(\n",
    "            max(batch_size/batch_target_rate - time_took_to_create_batch + 1, 0)\n",
    "        )\n",
    "\n",
    "    client.batch.configure(\n",
    "        batch_size=batch_size,\n",
    "        timeout_retries=5,\n",
    "        callback=callback,\n",
    "    )\n",
    "\n",
    "\n",
    "directory_path = 'data/coursematerial/test_data'\n",
    "import glob\n",
    "# Dictionary to hold file names and their elements\n",
    "\n",
    "# Find all PDF files in the specified directory\n",
    "pdf_files = glob.glob(os.path.join(directory_path, '*.pdf'))\n",
    "\n",
    "configure_batch(client, batch_size=10, batch_target_rate=1)\n",
    "\n",
    "for filename in pdf_files:\n",
    "    try:\n",
    "        elements = partition_pdf(filename=filename)\n",
    "        chunks = get_chunks(elements, 100, 500)\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    print(f\"Uploading {len(chunks)} chunks for {str(filename)}.\")\n",
    "    with client.batch as batch:\n",
    "        for chunk in chunks:\n",
    "            data_object = {\"content\": chunk['text'], \"filename\": filename}\n",
    "            print(data_object)\n",
    "            try:\n",
    "                print(\"Uploading chunk\")\n",
    "                batch.add_data_object(data_object=data_object, class_name=\"Test\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Failed to upload chunk for {str(filename)}.\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Aggregate': {'Test': [{'meta': {'count': 2}}]}}}\n"
     ]
    }
   ],
   "source": [
    "query2 = \"\"\"\n",
    "{\n",
    "  Aggregate {\n",
    "    Test {\n",
    "      meta {\n",
    "        count\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "result = client.query.raw(query2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": {\n",
      "        \"Get\": {\n",
      "            \"Test\": [\n",
      "                {\n",
      "                    \"filename\": \"data/coursematerial/test_data/lec01.pdf\"\n",
      "                },\n",
      "                {\n",
      "                    \"filename\": \"data/coursematerial/test_data/lec01.pdf\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 80.59%, threshold set to 80.00%\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate\",\"time\":\"2024-03-30T16:06:01-04:00\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "response = (\n",
    "    client.query\n",
    "    .get(\"Test\", [\"filename\"])\n",
    "    .with_limit(100)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift-6758",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
