{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.4\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "weaviate_version = weaviate.__version__\n",
    "print(weaviate_version)\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "import weaviate\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os\n",
    "import weaviate.classes as wvc\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import DataSourceMetadata\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from weaviate.util import generate_uuid5\n",
    "import time\n",
    "import glob\n",
    "from uuid import uuid5, NAMESPACE_DNS\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "WeaviateStartUpError",
     "evalue": "Embedded DB did not start because processes are already listening on ports http:8079 and grpc:50050use weaviate.connect_to_local(port=8079, grpc_port=50050) to connect to the existing instance",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWeaviateStartUpError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m weaviate\u001b[38;5;241m.\u001b[39mconnect_to_embedded(\n\u001b[1;32m      2\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# e.g. version=\"1.23.10\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-HuggingFace-Api-Key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_SzaiWGfpZEXDaqyfYcitHfXETTnpmUiMgg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Replace with your API key\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     },\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/connect/helpers.py:243\u001b[0m, in \u001b[0;36mconnect_to_embedded\u001b[0;34m(hostname, port, grpc_port, headers, additional_config, version, persistence_data_path, binary_path, environment_variables)\u001b[0m\n\u001b[1;32m    237\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_path \u001b[38;5;241m=\u001b[39m binary_path\n\u001b[1;32m    238\u001b[0m client \u001b[38;5;241m=\u001b[39m WeaviateClient(\n\u001b[1;32m    239\u001b[0m     embedded_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    240\u001b[0m     additional_headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    241\u001b[0m     additional_config\u001b[38;5;241m=\u001b[39madditional_config,\n\u001b[1;32m    242\u001b[0m )\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m __connect(client)\n",
      "File \u001b[0;32m~/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/connect/helpers.py:345\u001b[0m, in \u001b[0;36m__connect\u001b[0;34m(client)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    344\u001b[0m     client\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/connect/helpers.py:341\u001b[0m, in \u001b[0;36m__connect\u001b[0;34m(client)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__connect\u001b[39m(client: WeaviateClient) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m WeaviateClient:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 341\u001b[0m         client\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m client\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/client.py:282\u001b[0m, in \u001b[0;36mWeaviateClient.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_connected():\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__skip_init_checks)\n",
      "File \u001b[0;32m~/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/connect/v4.py:655\u001b[0m, in \u001b[0;36mConnectionV4.connect\u001b[0;34m(self, skip_init_checks)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, skip_init_checks: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mconnect(skip_init_checks)\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# create GRPC channel. If Weaviate does not support GRPC then error now.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_params\u001b[38;5;241m.\u001b[39m_grpc_channel(\n\u001b[1;32m    658\u001b[0m         async_channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxies\n\u001b[1;32m    659\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/connect/v4.py:135\u001b[0m, in \u001b[0;36m_Connection.connect\u001b[0;34m(self, skip_init_checks)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, skip_init_checks: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedded_db \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedded_db\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_clients(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auth, skip_init_checks)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/embedded.py:294\u001b[0m, in \u001b[0;36mEmbeddedV4.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m up \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_listening()\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m up[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m up[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateStartUpError(\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedded DB did not start because processes are already listening on ports http:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and grpc:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrpc_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse weaviate.connect_to_local(port=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, grpc_port=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mgrpc_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) to connect to the existing instance\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m up[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m up[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateStartUpError(\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedded DB did not start because a process is already listening on port http:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlook for another free port for the HTTP connection to you embedded instance\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m     )\n",
      "\u001b[0;31mWeaviateStartUpError\u001b[0m: Embedded DB did not start because processes are already listening on ports http:8079 and grpc:50050use weaviate.connect_to_local(port=8079, grpc_port=50050) to connect to the existing instance"
     ]
    }
   ],
   "source": [
    "client = weaviate.connect_to_embedded(\n",
    "    version=\"latest\",  # e.g. version=\"1.23.10\"\n",
    "    headers={\n",
    "        \"X-HuggingFace-Api-Key\": \"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\" # Replace with your API key\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed elements from data/test/UT_CS411/CSC311f19_final.pdf\n",
      "Processed elements from data/test/UT_CS411/CSC411f18_midterm2.pdf\n",
      "Processed elements from data/test/UT_CS411/CSC411f18_midterm1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the PDF files\n",
    "directory_path = 'data/test/UT_CS411'\n",
    "\n",
    "# Dictionary to hold file names and their elements\n",
    "files_elements = {}\n",
    "\n",
    "# Find all PDF files in the specified directory\n",
    "pdf_files = glob.glob(os.path.join(directory_path, '*.pdf'))\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    # Extract elements from the PDF\n",
    "    elements = partition_pdf(pdf_file)\n",
    "\n",
    "    \n",
    "    # Convert each element to a string (assuming each element has a .text attribute or similar)\n",
    "    # Adjust this line if the elements are structured differently\n",
    "    elements_text = [str(elem.text) if hasattr(elem, 'text') else str(elem) for elem in elements]\n",
    "    \n",
    "    # Extract the file name without extension to use as a key\n",
    "    file_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "    \n",
    "    # Store the elements in the dictionary\n",
    "    files_elements[file_name] = elements_text\n",
    "\n",
    "    print(f\"Processed elements from {pdf_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UT_CS411\n"
     ]
    }
   ],
   "source": [
    "directory= os.path.split(directory_path)[-1]\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For UT_CS411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_questions_refined(elements):\n",
    "    text = \"\\n\".join(str(elem) for elem in elements)\n",
    "    \n",
    "    # Patterns for main questions and subquestions\n",
    "    main_question_pattern = re.compile(r'(\\d+)\\.\\s*(.*?)\\s*(?=\\n\\d+\\.|$)', re.DOTALL)\n",
    "    subquestion_pattern = re.compile(r'\\n\\s*\\(([a-z])\\)\\s*(.*?)(?=\\n\\s*\\([a-z]\\)\\s*|\\n\\d+\\.|$)', re.DOTALL)\n",
    "    \n",
    "    questions = {}\n",
    "    \n",
    "    # Match and store main questions\n",
    "    for main_match in main_question_pattern.finditer(text):\n",
    "        main_question_number = main_match.group(1)\n",
    "        main_question_text = main_match.group(2).strip()\n",
    "        questions[main_question_number] = {\n",
    "            \"text\": main_question_text.split(\"\\n\", 1)[0],\n",
    "            \"subquestions\": []\n",
    "        }\n",
    "\n",
    "    # Match and store subquestions\n",
    "    for sq_match in subquestion_pattern.finditer(text):\n",
    "        subquestion_letter = sq_match.group(1).strip()\n",
    "        subquestion_text = sq_match.group(2).strip()\n",
    "\n",
    "        # Determine the appropriate main question for each subquestion\n",
    "        previous_main_question_number = None\n",
    "        for mq_number in questions:\n",
    "            if text.find(\"\\n\" + mq_number + \".\") < sq_match.start():\n",
    "                previous_main_question_number = mq_number\n",
    "\n",
    "        # Add the subquestion to the identified main question\n",
    "        if previous_main_question_number:\n",
    "            questions[previous_main_question_number][\"subquestions\"].append((subquestion_letter, subquestion_text))\n",
    "   \n",
    "    return questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_parse_questions_to_files_elements(files_elements):\n",
    "    # Structure to hold the parsed questions for each file\n",
    "    parsed_data = {}\n",
    "\n",
    "    # Iterate through each file's elements in the dictionary\n",
    "    for file_name, elements in files_elements.items():\n",
    "        # Apply the parse_questions function\n",
    "        parsed_questions = parse_questions_refined(elements)\n",
    "        \n",
    "        # Store the parsed questions for this file\n",
    "        parsed_data[file_name] = parsed_questions\n",
    "    \n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: CSC311f19_final\n",
      "Question 8: 5(cid:48)(cid:48) × 11(cid:48)(cid:48) or A4 aid sheets.\n",
      "Question 1: Is EM algorithm a supervised or an unsupervised learning method? Explain your answer.\n",
      "Question 2: How does EM algorithm and k-means compare? Write 3 similarities and 3 diﬀerences.\n",
      "Question 3: Explain why we call these steps expectation and maximization steps. What is it that we take expectation of and what is it that we maximize?\n",
      "Question 4: Principal Component Analysis. Recall that the optimal PCA subspace can be deter- mined from the eigendecomposition of the empirical covariance matrix ˆΣ. Also recall that the eigendecomposition can be expressed in terms of the following spectral decomposition of ˆΣ: ˆΣ = QΛQ(cid:62),\n",
      "Question 5: Support Vector Machines. Support vector machines learn a decision boundary leading to the largest margin from both classes. You are training SVM on a tiny dataset with 4 points shown below. This dataset consists of two examples with class label -1 (denoted with plus), and two examples with class label +1 (denoted with triangles).\n",
      "Question 6: Probabilistic Models . The Laplace distribution, parameterized by µ and β, is deﬁned as follows:\n",
      "Question 7: EM Algorithm.\n",
      "\n",
      "\n",
      "File: CSC411f18_midterm2\n",
      "Question 1: [1pt] Give one reason why an algorithm implemented in terms of matrix and vector operations can be faster than the same algorithm implemented in terms of for-loops.\n",
      "Question 2: [2pts] Brieﬂy explain two advantages of decision trees over K-nearest-neighbors.\n",
      "Question 3: [1pt] TRUE or FALSE: AdaBoost will eventually choose a weak classiﬁer that achieves a weighted error rate of 0. Brieﬂy justify your answer.\n",
      "Question 4: [1pt] In class, we considered using the squared Euclidean norm of the weights, (cid:107)w(cid:107)2, as a regularizer. Suppose we instead used the Euclidean norm (cid:107)w(cid:107) as a regularizer (i.e. without squaring it). Brieﬂy explain one way in which this would lead to diﬀerent behavior.\n",
      "Question 5: [2pts] Recall that hyperparameters are often tuned using a validation set.\n",
      " ----sub a: [1pt] Give an example of a hyperparameter which it is OK to tune on the training\n",
      "set (rather than a validation set). Brieﬂy explain your answer.\n",
      " ----sub b: [1pt] Give an example of a hyperparameter which should be tuned on a validation\n",
      "set, rather than the training set. Brieﬂy explain your answer.\n",
      "4\n",
      "CSC411/2515 Fall 2018\n",
      "Midterm Test, Version A\n",
      "Question 6: [2pts] In this question, you will write NumPy code to implement a 1-nearest-neighbour classiﬁer. Assume you are given an N × D data matrix X, where each row corresponds to one of the input vectors, and an integer array y with the corresponding labels. (You may assume the labels are integers from 1 to K.) You are given a query vector x_query. Your job is to return the predicted class (as an integer). Do not use a for-loop. If you don’t remember the API for a NumPy operation, then for partial credit, explain what you are trying to do.\n",
      "Question 7: [2pts] Consider the classiﬁcation problem with the following dataset:\n",
      " ----sub a: [1pt] Give the set of linear inequalities the weights and bias must satisfy.\n",
      " ----sub b: [1pt] Give a setting of the weights and bias that correctly classiﬁes all the training examples. You don’t need to show your work, but it might help you get partial credit.\n",
      "6\n",
      "CSC411/2515 Fall 2018\n",
      "Midterm Test, Version A\n",
      "Question 8: [2pts] Recall that in bagging, we compute an average of the predictions yavg = 1 m (cid:80)m\n",
      "Question 9: [2pts] Consider a regression problem where the input is a scalar x. Suppose we know that the dataset is generated by the following process. First, the target t is chosen from {0, 1} with equal probability. If t = 0, then x is sampled from a uniform distribution over the interval [1, 2]. If t = 1, then x is sampled from a uniform distribution over the interval [0, 2]. Give a function f (x), deﬁned for x ∈ [0, 2], such that y∗ = f (x) is the Bayes optimal predictor for t given x. (Note that even though t is binary valued, this is a regression problem, with squared error loss.)\n",
      "\n",
      "\n",
      "File: CSC411f18_midterm1\n",
      "Question 1: As discussed in lecture, when applying K-nearest-neighbors, it is common to normalize each input dimension to unit variance.\n",
      " ----sub a: [1pt] Why might it be advantageous to do this?\n",
      " ----sub b: [1pt] When might this normalization step not be a good idea? (Hint: You may want to consider the task of classifying images of handwritten digits, where the digit is centered within the image.)\n",
      "Question 2: [1pt] In random forests, what is the motivation for randomizing the set of attributes considered for each split?\n",
      "Question 3: [1pt] Suppose you want to evaluate the test error rate of a 1-nearest-neighbors classiﬁer. Assume you implement the algorithm the na¨ıve way, i.e. by explicitly computing all the distances and taking the min, rather than by using a fancy data structure. What is the running time of evaluating the test error? Give your answer in big-O notation, in terms of the number of training examples Ntrain, the number of test examples Ntest, and the input dimension D. Brieﬂy explain your answer.\n",
      "Question 4: (a) [1pt] Give one advantage of K-nearest-neighbors over linear regression.\n",
      " ----sub a: [1pt] Give one advantage of K-nearest-neighbors over linear regression.\n",
      " ----sub b: [1pt] Give one advantage of linear regression over K-nearest-neighbors.\n",
      "4\n",
      "CSC411/2515 Fall 2018\n",
      "Midterm Test, Version B\n",
      "Question 5: [1pt] Suppose linear regression (with squared error loss) is used as a classiﬁcation algorithm. TRUE or FALSE: if it correctly classiﬁes every training example, then its cost is zero. (By “cost”, we mean the function minimized during training.) Brieﬂy justify your answer.\n",
      "Question 6: [2pts] Let Z be a random variable and t be a real number. Show that\n",
      "Question 7: [2pts] Suppose binary-valued random variables X and Y have the following joint distribution:\n",
      "Question 8: [2pts] Recall that combining the logistic activation function with squared error loss suﬀers from saturation, whereby the gradient signal is very small when the prediction for a training example is very wrong. Logistic regression (i.e. logistic activation function with cross-entropy loss) doesn’t have this problem. Recall that the logistic function is deﬁned as σ(z) = 1/(1 + e−z). Now suppose we modify the activation function to squash the prediction y to be in the interval [0.1, 0.9], and then apply cross-entropy loss. I.e., z = w(cid:62)x + b y = 0.8σ(z) + 0.1\n",
      "Question 9: [2pts] Recall that the soft-margin SVM can be viewed as minimizing the hinge loss with an L2 regularization term. I.e.,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_name, elements in files_elements.items():\n",
    "    print(f\"File: {file_name}\")\n",
    "\n",
    "    # Apply the parse_questions function to the elements of the current file\n",
    "    parsed_questions = parse_questions_refined(elements)\n",
    "\n",
    "    # Iterate through the parsed questions and print them along with their subquestions\n",
    "    for question_num, question_info in parsed_questions.items():\n",
    "        print(f\"Question {question_num}: {question_info['text']}\")\n",
    "        for subq in question_info[\"subquestions\"]:\n",
    "            \n",
    "            print(f\" ----sub {subq[0]}: {subq[1]}\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard exam_content_fEZwRsuCsWDD in 4.051236ms\",\"time\":\"2024-04-12T14:30:04-04:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-04-12T14:30:04-04:00\",\"took\":117170}\n"
     ]
    }
   ],
   "source": [
    "client.collections.delete(name=\"exam_content\")\n",
    "\n",
    "\n",
    "questions = client.collections.create(\n",
    "\"exam_content\",\n",
    "vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_huggingface(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        vectorize_collection_name=True\n",
    "),\n",
    "properties=[  \n",
    "    wvc.config.Property(name=\"mainQuestion\", data_type=wvc.config.DataType.TEXT),\n",
    "    wvc.config.Property(name=\"subQuestion\", data_type=wvc.config.DataType.TEXT),\n",
    "    wvc.config.Property(name=\"filename\", data_type=wvc.config.DataType.TEXT),\n",
    "    # Add a 'course' property to store the course directory name\n",
    "    wvc.config.Property(name=\"course\", data_type=wvc.config.DataType.TEXT),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8': {'text': '5(cid:48)(cid:48) × 11(cid:48)(cid:48) or A4 aid sheets.', 'subquestions': []}, '1': {'text': 'Is EM algorithm a supervised or an unsupervised learning method? Explain your answer.', 'subquestions': []}, '2': {'text': 'How does EM algorithm and k-means compare? Write 3 similarities and 3 diﬀerences.', 'subquestions': []}, '3': {'text': 'Explain why we call these steps expectation and maximization steps. What is it that we take expectation of and what is it that we maximize?', 'subquestions': []}, '4': {'text': 'Principal Component Analysis. Recall that the optimal PCA subspace can be deter- mined from the eigendecomposition of the empirical covariance matrix ˆΣ. Also recall that the eigendecomposition can be expressed in terms of the following spectral decomposition of ˆΣ: ˆΣ = QΛQ(cid:62),', 'subquestions': []}, '5': {'text': 'Support Vector Machines. Support vector machines learn a decision boundary leading to the largest margin from both classes. You are training SVM on a tiny dataset with 4 points shown below. This dataset consists of two examples with class label -1 (denoted with plus), and two examples with class label +1 (denoted with triangles).', 'subquestions': []}, '6': {'text': 'Probabilistic Models . The Laplace distribution, parameterized by µ and β, is deﬁned as follows:', 'subquestions': []}, '7': {'text': 'EM Algorithm.', 'subquestions': []}}\n",
      "importing question: 8\n",
      "importing question: 1\n",
      "importing question: 2\n",
      "importing question: 3\n",
      "importing question: 4\n",
      "importing question: 5\n",
      "importing question: 6\n",
      "importing question: 7\n",
      "{'1': {'text': '[1pt] Give one reason why an algorithm implemented in terms of matrix and vector operations can be faster than the same algorithm implemented in terms of for-loops.', 'subquestions': []}, '2': {'text': '[2pts] Brieﬂy explain two advantages of decision trees over K-nearest-neighbors.', 'subquestions': []}, '3': {'text': '[1pt] TRUE or FALSE: AdaBoost will eventually choose a weak classiﬁer that achieves a weighted error rate of 0. Brieﬂy justify your answer.', 'subquestions': []}, '4': {'text': '[1pt] In class, we considered using the squared Euclidean norm of the weights, (cid:107)w(cid:107)2, as a regularizer. Suppose we instead used the Euclidean norm (cid:107)w(cid:107) as a regularizer (i.e. without squaring it). Brieﬂy explain one way in which this would lead to diﬀerent behavior.', 'subquestions': []}, '5': {'text': '[2pts] Recall that hyperparameters are often tuned using a validation set.', 'subquestions': [('a', '[1pt] Give an example of a hyperparameter which it is OK to tune on the training\\nset (rather than a validation set). Brieﬂy explain your answer.'), ('b', '[1pt] Give an example of a hyperparameter which should be tuned on a validation\\nset, rather than the training set. Brieﬂy explain your answer.\\n4\\nCSC411/2515 Fall 2018\\nMidterm Test, Version A')]}, '6': {'text': '[2pts] In this question, you will write NumPy code to implement a 1-nearest-neighbour classiﬁer. Assume you are given an N × D data matrix X, where each row corresponds to one of the input vectors, and an integer array y with the corresponding labels. (You may assume the labels are integers from 1 to K.) You are given a query vector x_query. Your job is to return the predicted class (as an integer). Do not use a for-loop. If you don’t remember the API for a NumPy operation, then for partial credit, explain what you are trying to do.', 'subquestions': []}, '7': {'text': '[2pts] Consider the classiﬁcation problem with the following dataset:', 'subquestions': [('a', '[1pt] Give the set of linear inequalities the weights and bias must satisfy.'), ('b', '[1pt] Give a setting of the weights and bias that correctly classiﬁes all the training examples. You don’t need to show your work, but it might help you get partial credit.\\n6\\nCSC411/2515 Fall 2018\\nMidterm Test, Version A')]}, '8': {'text': '[2pts] Recall that in bagging, we compute an average of the predictions yavg = 1 m (cid:80)m', 'subquestions': []}, '9': {'text': '[2pts] Consider a regression problem where the input is a scalar x. Suppose we know that the dataset is generated by the following process. First, the target t is chosen from {0, 1} with equal probability. If t = 0, then x is sampled from a uniform distribution over the interval [1, 2]. If t = 1, then x is sampled from a uniform distribution over the interval [0, 2]. Give a function f (x), deﬁned for x ∈ [0, 2], such that y∗ = f (x) is the Bayes optimal predictor for t given x. (Note that even though t is binary valued, this is a regression problem, with squared error loss.)', 'subquestions': []}}\n",
      "importing question: 1\n",
      "importing question: 2\n",
      "importing question: 3\n",
      "importing question: 4\n",
      "importing question: 5\n",
      "importing question: 6\n",
      "importing question: 7\n",
      "importing question: 8\n",
      "importing question: 9\n",
      "{'1': {'text': 'As discussed in lecture, when applying K-nearest-neighbors, it is common to normalize each input dimension to unit variance.', 'subquestions': [('a', '[1pt] Why might it be advantageous to do this?'), ('b', '[1pt] When might this normalization step not be a good idea? (Hint: You may want to consider the task of classifying images of handwritten digits, where the digit is centered within the image.)')]}, '2': {'text': '[1pt] In random forests, what is the motivation for randomizing the set of attributes considered for each split?', 'subquestions': []}, '3': {'text': '[1pt] Suppose you want to evaluate the test error rate of a 1-nearest-neighbors classiﬁer. Assume you implement the algorithm the na¨ıve way, i.e. by explicitly computing all the distances and taking the min, rather than by using a fancy data structure. What is the running time of evaluating the test error? Give your answer in big-O notation, in terms of the number of training examples Ntrain, the number of test examples Ntest, and the input dimension D. Brieﬂy explain your answer.', 'subquestions': []}, '4': {'text': '(a) [1pt] Give one advantage of K-nearest-neighbors over linear regression.', 'subquestions': [('a', '[1pt] Give one advantage of K-nearest-neighbors over linear regression.'), ('b', '[1pt] Give one advantage of linear regression over K-nearest-neighbors.\\n4\\nCSC411/2515 Fall 2018\\nMidterm Test, Version B')]}, '5': {'text': '[1pt] Suppose linear regression (with squared error loss) is used as a classiﬁcation algorithm. TRUE or FALSE: if it correctly classiﬁes every training example, then its cost is zero. (By “cost”, we mean the function minimized during training.) Brieﬂy justify your answer.', 'subquestions': []}, '6': {'text': '[2pts] Let Z be a random variable and t be a real number. Show that', 'subquestions': []}, '7': {'text': '[2pts] Suppose binary-valued random variables X and Y have the following joint distribution:', 'subquestions': []}, '8': {'text': '[2pts] Recall that combining the logistic activation function with squared error loss suﬀers from saturation, whereby the gradient signal is very small when the prediction for a training example is very wrong. Logistic regression (i.e. logistic activation function with cross-entropy loss) doesn’t have this problem. Recall that the logistic function is deﬁned as σ(z) = 1/(1 + e−z). Now suppose we modify the activation function to squash the prediction y to be in the interval [0.1, 0.9], and then apply cross-entropy loss. I.e., z = w(cid:62)x + b y = 0.8σ(z) + 0.1', 'subquestions': []}, '9': {'text': '[2pts] Recall that the soft-margin SVM can be viewed as minimizing the hinge loss with an L2 regularization term. I.e.,', 'subquestions': []}}\n",
      "importing question: 1\n",
      "importing question: 2\n",
      "importing question: 3\n",
      "importing question: 4\n",
      "importing question: 5\n",
      "importing question: 6\n",
      "importing question: 7\n",
      "importing question: 8\n",
      "importing question: 9\n"
     ]
    }
   ],
   "source": [
    "question_objs = list()\n",
    "count = 0\n",
    "for file_name, elements in files_elements.items():\n",
    "\n",
    "    parsed_questions = parse_questions_refined(elements)\n",
    "    print(parsed_questions)\n",
    "    \n",
    "    for question, details in parsed_questions.items():  # Batch import data\n",
    "        print(f\"importing question: {question}\")\n",
    "        if len(details[\"subquestions\"]) == 0:  # if there are no subquestions\n",
    "            properties = {\n",
    "                \"mainQuestion\": question,\n",
    "                \"subQuestion\": \"\",\n",
    "                \"filename\": file_name,\n",
    "                \"course\": directory\n",
    "            }\n",
    "            question_objs.append(properties)\n",
    "        else: \n",
    "            for subq in details[\"subquestions\"]:\n",
    "                properties = {\n",
    "                    \"mainQuestion\": question,\n",
    "                    \"subQuestion\": subq[1],\n",
    "                    \"filename\": file_name,\n",
    "                    \"course\": directory\n",
    "                }\n",
    "                question_objs.append(properties)\n",
    "        count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 26 questions\n"
     ]
    }
   ],
   "source": [
    "print(f\"Importing {count} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 26\n"
     ]
    }
   ],
   "source": [
    "# ===== Batch import =====\n",
    "with questions.batch.dynamic() as batch:\n",
    "    for data_row in question_objs:\n",
    "        obj_uuid = uuid5(NAMESPACE_DNS, data_row[\"mainQuestion\"]+ data_row[\"filename\"])\n",
    "        batch.add_object(\n",
    "            properties=data_row,\n",
    "            uuid=obj_uuid\n",
    "        )\n",
    "        \n",
    "print(\"Total chunks:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks in Wv: 26\n"
     ]
    }
   ],
   "source": [
    "lecture_content = client.collections.get(\"exam_content\")\n",
    "response_ttl = lecture_content.aggregate.over_all(total_count=True)\n",
    "\n",
    "print( \"Total Chunks in Wv:\",response_ttl.total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift-6758",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
