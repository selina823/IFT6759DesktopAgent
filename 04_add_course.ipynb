{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured_pytesseract in /Users/selina/anaconda3/lib/python3.11/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_pytesseract) (10.2.0)\n",
      "Requirement already satisfied: pytesseract in /Users/selina/anaconda3/lib/python3.11/site-packages (0.3.10)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pytesseract) (10.2.0)\n",
      "Requirement already satisfied: unstructured_inference in /Users/selina/anaconda3/lib/python3.11/site-packages (0.7.24)\n",
      "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (0.3.4)\n",
      "Requirement already satisfied: python-multipart in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (0.0.9)\n",
      "Requirement already satisfied: huggingface-hub in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (0.15.1)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (4.9.0.80)\n",
      "Requirement already satisfied: onnx in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (1.15.0)\n",
      "Requirement already satisfied: onnxruntime<1.16 in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (1.15.1)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (4.32.1)\n",
      "Requirement already satisfied: rapidfuzz in /Users/selina/anaconda3/lib/python3.11/site-packages (from unstructured_inference) (3.6.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/selina/anaconda3/lib/python3.11/site-packages (from onnxruntime<1.16->unstructured_inference) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/selina/anaconda3/lib/python3.11/site-packages (from onnxruntime<1.16->unstructured_inference) (24.3.7)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/selina/anaconda3/lib/python3.11/site-packages (from onnxruntime<1.16->unstructured_inference) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/selina/anaconda3/lib/python3.11/site-packages (from onnxruntime<1.16->unstructured_inference) (23.2)\n",
      "Requirement already satisfied: protobuf in /Users/selina/anaconda3/lib/python3.11/site-packages (from onnxruntime<1.16->unstructured_inference) (4.25.3)\n",
      "Requirement already satisfied: sympy in /Users/selina/anaconda3/lib/python3.11/site-packages (from onnxruntime<1.16->unstructured_inference) (1.11.1)\n",
      "Requirement already satisfied: filelock in /Users/selina/anaconda3/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured_inference) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured_inference) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/selina/anaconda3/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured_inference) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/selina/anaconda3/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured_inference) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured_inference) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured_inference) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/selina/anaconda3/lib/python3.11/site-packages (from transformers>=4.25.1->unstructured_inference) (4.66.2)\n",
      "Requirement already satisfied: fsspec in /Users/selina/anaconda3/lib/python3.11/site-packages (from huggingface-hub->unstructured_inference) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/selina/anaconda3/lib/python3.11/site-packages (from huggingface-hub->unstructured_inference) (4.9.0)\n",
      "Requirement already satisfied: scipy in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.11.1)\n",
      "Requirement already satisfied: pandas in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.1.4)\n",
      "Requirement already satisfied: pillow in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (10.2.0)\n",
      "Requirement already satisfied: iopath in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.1.10)\n",
      "Requirement already satisfied: pdfplumber in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.11.0)\n",
      "Requirement already satisfied: pdf2image in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.17.0)\n",
      "Requirement already satisfied: torch in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.1.1)\n",
      "Requirement already satisfied: torchvision in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.16.1)\n",
      "Requirement already satisfied: effdet in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.4.1)\n",
      "Requirement already satisfied: pytesseract in /Users/selina/anaconda3/lib/python3.11/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.3.10)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from coloredlogs->onnxruntime<1.16->unstructured_inference) (10.0)\n",
      "Requirement already satisfied: timm>=0.9.2 in /Users/selina/anaconda3/lib/python3.11/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.9.16)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /Users/selina/anaconda3/lib/python3.11/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.0.7)\n",
      "Requirement already satisfied: omegaconf>=2.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.3.0)\n",
      "Requirement already satisfied: networkx in /Users/selina/anaconda3/lib/python3.11/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/selina/anaconda3/lib/python3.11/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.1.2)\n",
      "Requirement already satisfied: portalocker in /Users/selina/anaconda3/lib/python3.11/site-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2023.3)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (20231228)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (4.28.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (41.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/selina/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.25.1->unstructured_inference) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.25.1->unstructured_inference) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/selina/anaconda3/lib/python3.11/site-packages (from requests->transformers>=4.25.1->unstructured_inference) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/selina/anaconda3/lib/python3.11/site-packages (from sympy->onnxruntime<1.16->unstructured_inference) (1.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/selina/anaconda3/lib/python3.11/site-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/selina/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.1.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/selina/anaconda3/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/selina/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/selina/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/selina/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured_inference) (3.0.9)\n",
      "Requirement already satisfied: pycparser in /Users/selina/anaconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured_inference) (2.21)\n"
     ]
    }
   ],
   "source": [
    "#### BE SURE TO install the following packages before running the code####\n",
    "! pip install unstructured_pytesseract\n",
    "! pip install pytesseract\n",
    "! pip install unstructured_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BE SURE TO install the aboving packages before running the code####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.1\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "weaviate_version = weaviate.__version__\n",
    "print(weaviate_version)\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "import weaviate\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os\n",
    "import weaviate.classes as wvc\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import DataSourceMetadata\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from weaviate.util import generate_uuid5\n",
    "import time\n",
    "import glob\n",
    "from uuid import uuid5, NAMESPACE_DNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weaviate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m weaviate\u001b[38;5;241m.\u001b[39mconnect_to_embedded(\n\u001b[1;32m      2\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# e.g. version=\"1.23.10\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-HuggingFace-Api-Key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_SzaiWGfpZEXDaqyfYcitHfXETTnpmUiMgg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Replace with your API key\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     },\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weaviate' is not defined"
     ]
    }
   ],
   "source": [
    "client = weaviate.connect_to_embedded(\n",
    "    version=\"latest\",  # e.g. version=\"1.23.10\"\n",
    "    headers={\n",
    "        \"X-HuggingFace-Api-Key\": \"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\" # Replace with your API key\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(elements, chunk_under_n_chars=500, chunk_new_after_n_chars=1500):\n",
    "\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        multipage_sections=False, # If True, the title of the first page is used for all pages\n",
    "        combine_text_under_n_chars=chunk_under_n_chars,\n",
    "        new_after_n_chars=chunk_new_after_n_chars\n",
    " \n",
    "    )\n",
    "\n",
    "    for i in range(len(chunks)):\n",
    "        chunks[i] = {\"text\": chunks[i].text, \"filename\": chunks[i].metadata.filename}\n",
    "\n",
    "    chunk_texts = [x['text'] for x in chunks]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/coursematerial/UBC_MT307/lec01.pdf', 'data/coursematerial/UBC_MT307/lec02.pdf', 'data/coursematerial/UBC_MT307/lec03.pdf', 'data/coursematerial/UBC_MT307/lec04.pdf', 'data/coursematerial/UBC_MT307/lec05.pdf', 'data/coursematerial/UBC_MT307/lec06.pdf', 'data/coursematerial/UBC_MT307/lec07.pdf', 'data/coursematerial/UBC_MT307/lec08.pdf', 'data/coursematerial/UBC_MT307/lec09.pdf', 'data/coursematerial/UBC_MT307/lec10.pdf', 'data/coursematerial/UBC_MT307/lec11.pdf', 'data/coursematerial/UBC_MT307/lec12.pdf', 'data/coursematerial/UBC_MT307/lec13.pdf', 'data/coursematerial/UBC_MT307/lec14.pdf', 'data/coursematerial/UBC_MT307/lec15.pdf', 'data/coursematerial/UBC_MT307/lec16.pdf', 'data/coursematerial/UBC_MT307/lec17.pdf', 'data/coursematerial/UBC_MT307/lec18.pdf', 'data/coursematerial/UBC_MT307/lec19.pdf', 'data/coursematerial/UBC_MT307/lec20.pdf', 'data/coursematerial/UBC_MT307/lec21.pdf', 'data/coursematerial/UBC_MT307/lec22.pdf']\n",
      "['data/coursematerial/UT_CS411/lec01.pdf', 'data/coursematerial/UT_CS411/lec02.pdf', 'data/coursematerial/UT_CS411/lec03.pdf', 'data/coursematerial/UT_CS411/lec04.pdf', 'data/coursematerial/UT_CS411/lec05.pdf', 'data/coursematerial/UT_CS411/lec06.pdf', 'data/coursematerial/UT_CS411/lec07.pdf', 'data/coursematerial/UT_CS411/lec08.pdf', 'data/coursematerial/UT_CS411/lec09.pdf', 'data/coursematerial/UT_CS411/lec10.pdf', 'data/coursematerial/UT_CS411/lec11.pdf', 'data/coursematerial/UT_CS411/lec12.pdf']\n",
      "['data/coursematerial/UBC_ST302/slides1.pdf', 'data/coursematerial/UBC_ST302/slides10.pdf', 'data/coursematerial/UBC_ST302/slides11.pdf', 'data/coursematerial/UBC_ST302/slides12.pdf', 'data/coursematerial/UBC_ST302/slides13.pdf', 'data/coursematerial/UBC_ST302/slides14.pdf', 'data/coursematerial/UBC_ST302/slides15.pdf', 'data/coursematerial/UBC_ST302/slides16.pdf', 'data/coursematerial/UBC_ST302/slides17.pdf', 'data/coursematerial/UBC_ST302/slides18.pdf', 'data/coursematerial/UBC_ST302/slides19.pdf', 'data/coursematerial/UBC_ST302/slides2.pdf', 'data/coursematerial/UBC_ST302/slides20.pdf', 'data/coursematerial/UBC_ST302/slides21.pdf', 'data/coursematerial/UBC_ST302/slides22.pdf', 'data/coursematerial/UBC_ST302/slides23.pdf', 'data/coursematerial/UBC_ST302/slides3.pdf', 'data/coursematerial/UBC_ST302/slides4.pdf', 'data/coursematerial/UBC_ST302/slides5.pdf', 'data/coursematerial/UBC_ST302/slides6.pdf', 'data/coursematerial/UBC_ST302/slides7.pdf', 'data/coursematerial/UBC_ST302/slides8.pdf', 'data/coursematerial/UBC_ST302/slides9.pdf']\n"
     ]
    }
   ],
   "source": [
    "directory_path = 'data/coursematerial'\n",
    "for subdir, dirs, files in os.walk(directory_path):\n",
    "    for directory in dirs:\n",
    "        course_path = os.path.join(subdir, directory)\n",
    "        pdf_files = glob.glob(os.path.join(course_path, '*.pdf'))\n",
    "        pdf_files.sort()\n",
    "        print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard lecture_content_nTDlm3zyJUet in 2.109042ms\",\"time\":\"2024-04-28T19:26:29-04:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-04-28T19:26:29-04:00\",\"took\":124375}\n"
     ]
    }
   ],
   "source": [
    "client.collections.delete(name=\"lecture_content\")\n",
    "directory_path = 'data/coursematerial'\n",
    "\n",
    "questions = client.collections.create(\n",
    "\"lecture_content\",\n",
    "vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_huggingface(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        vectorize_collection_name=True\n",
    "),\n",
    "properties=[  \n",
    "    wvc.config.Property(name=\"content\", data_type=wvc.config.DataType.TEXT),\n",
    "    wvc.config.Property(name=\"filename\", data_type=wvc.config.DataType.TEXT),\n",
    "    # Add a 'course' property to store the course directory name\n",
    "    wvc.config.Property(name=\"course\", data_type=wvc.config.DataType.TEXT),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement poppler (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for poppler\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 chunks found in data/coursematerial/UBC_MT307/lec01.pdf\n",
      "\n",
      "6 chunks found in data/coursematerial/UBC_MT307/lec02.pdf\n",
      "\n",
      "24 chunks found in data/coursematerial/UBC_MT307/lec03.pdf\n",
      "\n",
      "8 chunks found in data/coursematerial/UBC_MT307/lec04.pdf\n",
      "\n",
      "55 chunks found in data/coursematerial/UBC_MT307/lec05.pdf\n",
      "\n",
      "28 chunks found in data/coursematerial/UBC_MT307/lec06.pdf\n",
      "\n",
      "10 chunks found in data/coursematerial/UBC_MT307/lec07.pdf\n",
      "\n",
      "7 chunks found in data/coursematerial/UBC_MT307/lec08.pdf\n",
      "\n",
      "24 chunks found in data/coursematerial/UBC_MT307/lec09.pdf\n",
      "\n",
      "17 chunks found in data/coursematerial/UBC_MT307/lec10.pdf\n",
      "\n",
      "1 chunks found in data/coursematerial/UBC_MT307/lec11.pdf\n",
      "\n",
      "1 chunks found in data/coursematerial/UBC_MT307/lec12.pdf\n",
      "\n",
      "9 chunks found in data/coursematerial/UBC_MT307/lec13.pdf\n",
      "\n",
      "1 chunks found in data/coursematerial/UBC_MT307/lec14.pdf\n",
      "\n",
      "16 chunks found in data/coursematerial/UBC_MT307/lec15.pdf\n",
      "\n",
      "29 chunks found in data/coursematerial/UBC_MT307/lec16.pdf\n",
      "\n",
      "1 chunks found in data/coursematerial/UBC_MT307/lec17.pdf\n",
      "\n",
      "20 chunks found in data/coursematerial/UBC_MT307/lec18.pdf\n",
      "\n",
      "11 chunks found in data/coursematerial/UBC_MT307/lec19.pdf\n",
      "\n",
      "19 chunks found in data/coursematerial/UBC_MT307/lec20.pdf\n",
      "\n",
      "44 chunks found in data/coursematerial/UBC_MT307/lec21.pdf\n",
      "\n",
      "7 chunks found in data/coursematerial/UBC_MT307/lec22.pdf\n",
      "\n",
      "103 chunks found in data/coursematerial/UT_CS411/lec01.pdf\n",
      "\n",
      "102 chunks found in data/coursematerial/UT_CS411/lec02.pdf\n",
      "\n",
      "108 chunks found in data/coursematerial/UT_CS411/lec03.pdf\n",
      "\n",
      "249 chunks found in data/coursematerial/UT_CS411/lec04.pdf\n",
      "\n",
      "111 chunks found in data/coursematerial/UT_CS411/lec05.pdf\n",
      "\n",
      "179 chunks found in data/coursematerial/UT_CS411/lec06.pdf\n",
      "\n",
      "77 chunks found in data/coursematerial/UT_CS411/lec07.pdf\n",
      "\n",
      "120 chunks found in data/coursematerial/UT_CS411/lec08.pdf\n",
      "\n",
      "87 chunks found in data/coursematerial/UT_CS411/lec09.pdf\n",
      "\n",
      "95 chunks found in data/coursematerial/UT_CS411/lec10.pdf\n",
      "\n",
      "91 chunks found in data/coursematerial/UT_CS411/lec11.pdf\n",
      "\n",
      "149 chunks found in data/coursematerial/UT_CS411/lec12.pdf\n",
      "\n",
      "44 chunks found in data/coursematerial/UBC_ST302/slides1.pdf\n",
      "\n",
      "74 chunks found in data/coursematerial/UBC_ST302/slides10.pdf\n",
      "\n",
      "51 chunks found in data/coursematerial/UBC_ST302/slides11.pdf\n",
      "\n",
      "67 chunks found in data/coursematerial/UBC_ST302/slides12.pdf\n",
      "\n",
      "32 chunks found in data/coursematerial/UBC_ST302/slides13.pdf\n",
      "\n",
      "53 chunks found in data/coursematerial/UBC_ST302/slides14.pdf\n",
      "\n",
      "56 chunks found in data/coursematerial/UBC_ST302/slides15.pdf\n",
      "\n",
      "25 chunks found in data/coursematerial/UBC_ST302/slides16.pdf\n",
      "\n",
      "31 chunks found in data/coursematerial/UBC_ST302/slides17.pdf\n",
      "\n",
      "44 chunks found in data/coursematerial/UBC_ST302/slides18.pdf\n",
      "\n",
      "46 chunks found in data/coursematerial/UBC_ST302/slides19.pdf\n",
      "\n",
      "59 chunks found in data/coursematerial/UBC_ST302/slides2.pdf\n",
      "\n",
      "43 chunks found in data/coursematerial/UBC_ST302/slides20.pdf\n",
      "\n",
      "62 chunks found in data/coursematerial/UBC_ST302/slides21.pdf\n",
      "\n",
      "33 chunks found in data/coursematerial/UBC_ST302/slides22.pdf\n",
      "\n",
      "49 chunks found in data/coursematerial/UBC_ST302/slides23.pdf\n",
      "\n",
      "29 chunks found in data/coursematerial/UBC_ST302/slides3.pdf\n",
      "\n",
      "84 chunks found in data/coursematerial/UBC_ST302/slides4.pdf\n",
      "\n",
      "74 chunks found in data/coursematerial/UBC_ST302/slides5.pdf\n",
      "\n",
      "49 chunks found in data/coursematerial/UBC_ST302/slides6.pdf\n",
      "\n",
      "46 chunks found in data/coursematerial/UBC_ST302/slides7.pdf\n",
      "\n",
      "34 chunks found in data/coursematerial/UBC_ST302/slides8.pdf\n",
      "\n",
      "35 chunks found in data/coursematerial/UBC_ST302/slides9.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_chunks = 0\n",
    "question_objs = list()\n",
    "\n",
    "# Modified to include subdirectories (each representing a course), \n",
    "# write the output to a txt file \n",
    "output_file_path = \"wvs_output.txt\"\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    for subdir, dirs, files in os.walk(directory_path):\n",
    "        for directory in dirs:\n",
    "            course_path = os.path.join(subdir, directory)\n",
    "            pdf_files = glob.glob(os.path.join(course_path, '*.pdf'))\n",
    "            pdf_files.sort()\n",
    "            \n",
    "            for filename in pdf_files:\n",
    "                try:\n",
    "                    elements = partition_pdf(filename=filename)\n",
    "                    chunks = get_chunks(elements, 100, 1000)\n",
    "                except IndexError as e:\n",
    "                    output_file.write(f\"{e}\\n\")\n",
    "                    continue\n",
    "\n",
    "                # print(len(chunks), \"chunks found in\", filename)\n",
    "                output_message = f\"{len(chunks)} chunks found in {filename}\\n\"\n",
    "                print(output_message)  \n",
    "                output_file.write(output_message)\n",
    "                \n",
    "                total_chunks += len(chunks)\n",
    "                for chunk in chunks:\n",
    "                    # Include the 'course' property in your object\n",
    "                    question_objs.append({\n",
    "                        \"content\": chunk['text'],\n",
    "                        \"filename\": os.path.basename(filename),\n",
    "                        # use 'directory' as the course ID, e.g., 'UT_CS411'\n",
    "                        \"course\": directory,  \n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2952\n"
     ]
    }
   ],
   "source": [
    "# ===== Batch import =====\n",
    "with questions.batch.dynamic() as batch:\n",
    "    for data_row in question_objs:\n",
    "        obj_uuid = uuid5(NAMESPACE_DNS, data_row[\"content\"][:500] + data_row[\"filename\"])  #user performance optimization link hyperparameter. \n",
    "        batch.add_object(\n",
    "            properties=data_row,\n",
    "            uuid=obj_uuid\n",
    "        )\n",
    "        \n",
    "print(\"Total chunks:\", total_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks in Wv: 2694\n"
     ]
    }
   ],
   "source": [
    "lecture_content = client.collections.get(\"lecture_content\")\n",
    "response_ttl = lecture_content.aggregate.over_all(total_count=True)\n",
    "\n",
    "print( \"Total Chunks in Wv:\",response_ttl.total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Why not R?\\n\\nI don’t know R. The machine learning community mostly uses Python. Follow-up courses like STA414 typically use Python.\\n\\nIntro ML (UofT)\\n\\nSTA314-Lec1\\n\\n6 / 65'}\n"
     ]
    }
   ],
   "source": [
    "x = lecture_content.query.fetch_objects(\n",
    "    limit=4000,\n",
    "    return_properties=[\"content\"],\n",
    "    filters=Filter.by_property(\"course\").contains_any([\"UT_CS411\"]),\n",
    ")\n",
    "print(x.objects[0].properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: lec16.pdf, Course: UBC_MT307, Count: 29\n",
      "Filename: slides15.pdf, Course: UBC_ST302, Count: 56\n",
      "Filename: lec03.pdf, Course: UT_CS411, Count: 106\n",
      "Filename: slides21.pdf, Course: UBC_ST302, Count: 62\n",
      "Filename: slides14.pdf, Course: UBC_ST302, Count: 33\n",
      "Filename: lec05.pdf, Course: UBC_MT307, Count: 142\n",
      "Filename: slides4.pdf, Course: UBC_ST302, Count: 84\n",
      "Filename: lec01.pdf, Course: UT_CS411, Count: 126\n",
      "Filename: lec04.pdf, Course: UT_CS411, Count: 147\n",
      "Filename: slides17.pdf, Course: UBC_ST302, Count: 31\n",
      "Filename: lec02.pdf, Course: UT_CS411, Count: 106\n",
      "Filename: slides2.pdf, Course: UBC_ST302, Count: 59\n",
      "Filename: slides19.pdf, Course: UBC_ST302, Count: 46\n",
      "Filename: lec21.pdf, Course: UBC_MT307, Count: 44\n",
      "Filename: slides5.pdf, Course: UBC_ST302, Count: 74\n",
      "Filename: lec10.pdf, Course: UBC_MT307, Count: 17\n",
      "Filename: slides16.pdf, Course: UBC_ST302, Count: 25\n",
      "Filename: slides7.pdf, Course: UBC_ST302, Count: 46\n",
      "Filename: slides6.pdf, Course: UBC_ST302, Count: 49\n",
      "Filename: slides3.pdf, Course: UBC_ST302, Count: 29\n",
      "Filename: slides20.pdf, Course: UBC_ST302, Count: 43\n",
      "Filename: lec06.pdf, Course: UBC_MT307, Count: 71\n",
      "Filename: lec08.pdf, Course: UBC_MT307, Count: 7\n",
      "Filename: slides8.pdf, Course: UBC_ST302, Count: 34\n",
      "Filename: slides23.pdf, Course: UBC_ST302, Count: 49\n",
      "Filename: lec20.pdf, Course: UBC_MT307, Count: 19\n",
      "Filename: lec13.pdf, Course: UBC_MT307, Count: 9\n",
      "Filename: lec07.pdf, Course: UBC_MT307, Count: 10\n",
      "Filename: slides18.pdf, Course: UBC_ST302, Count: 44\n",
      "Filename: slides22.pdf, Course: UBC_ST302, Count: 33\n",
      "Filename: slides9.pdf, Course: UBC_ST302, Count: 35\n",
      "Filename: lec19.pdf, Course: UBC_MT307, Count: 11\n",
      "Filename: lec15.pdf, Course: UBC_MT307, Count: 16\n",
      "Filename: lec09.pdf, Course: UBC_MT307, Count: 24\n",
      "Filename: lec22.pdf, Course: UBC_MT307, Count: 7\n",
      "Filename: lec18.pdf, Course: UBC_MT307, Count: 20\n",
      "Filename: lec14.pdf, Course: UBC_MT307, Count: 1\n",
      "Filename: lec12.pdf, Course: UBC_MT307, Count: 1\n",
      "Filename: lec17.pdf, Course: UBC_MT307, Count: 1\n",
      "Filename: lec11.pdf, Course: UBC_MT307, Count: 1\n",
      "Total courses queried: 40\n"
     ]
    }
   ],
   "source": [
    "# ===== query data =====\n",
    "\n",
    "lecture_content = client.collections.get(\"lecture_content\")\n",
    "response = lecture_content.query.fetch_objects(\n",
    "    limit=4000,\n",
    "    return_properties=[\"filename\", \"course\"]\n",
    ")\n",
    "\n",
    "filename_counts = {}\n",
    "filename_courses = {}  \n",
    "\n",
    "for o in response.objects:\n",
    "    filename = o.properties['filename']\n",
    "    course = o.properties.get('course', 'Unknown course')  \n",
    "    \n",
    "    # Update counts\n",
    "    filename_counts[filename] = filename_counts.get(filename, 0) + 1\n",
    "    \n",
    "    # Update courses (assuming a filename only belongs to one course for simplicity)\n",
    "    if filename not in filename_courses:\n",
    "        filename_courses[filename] = course\n",
    "\n",
    "filename_list = []\n",
    "course_list = []\n",
    "# Now, print filename, course, and count\n",
    "for filename, count in filename_counts.items():\n",
    "    course = filename_courses.get(filename, 'Unknown course')  # Get the course associated with the filename\n",
    "    filename_list.append(filename)\n",
    "    course_list.append(course)\n",
    "    print(f\"Filename: {filename}, Course: {course}, Count: {count}\")\n",
    "\n",
    "# total courses queried \n",
    "print(\"Total courses queried:\", len(filename_courses.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Shutting down... \",\"time\":\"2024-04-28T22:17:31-04:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Stopped serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-04-28T22:17:31-04:00\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"telemetry_push\",\"level\":\"info\",\"msg\":\"telemetry terminated\",\"payload\":\"\\u0026{MachineID:0a04ea07-42c6-4101-9d64-8eaa9383adc0 Type:TERMINATE Version:1.24.8 Modules:generative-openai,qna-openai,ref2vec-centroid,reranker-cohere,text2vec-cohere,text2vec-huggingface,text2vec-openai NumObjects:2704 OS:darwin Arch:arm64}\",\"time\":\"2024-04-28T22:17:32-04:00\"}\n"
     ]
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = client.collections.get(\"lecture_content\")\n",
    "\n",
    "response = questions.query.near_text(\n",
    "    query=\"Support Vector Machine\",\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "print(response.objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your API key here\n",
    "key = 'your_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client_open_ai = OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauss elimination,Pivot\n",
      "Normal Approximation, Probability\n",
      "Bias-Variance Decomposition, Supervised Learning\n",
      "Conditional Expectation, Joint Probability Distribution\n",
      "Variance, Expectation\n",
      "Vector calculator, Reverse Polish Notation\n",
      "Axioms of Probability, Examples\n",
      "PCA, Supervised Learning\n",
      "Bagging, Linear Regression\n",
      "Jointly Distributed Random Variables, Joint Probability Density Function\n"
     ]
    }
   ],
   "source": [
    "topic_list = []\n",
    "for i in range(len(filename_list)):\n",
    "# for i in range(10):\n",
    "    filename = filename_list[i]\n",
    "    course = course_list[i]\n",
    "    system_message_2 = \"You will be provided with a list of course material (delimited with <content> XML tags) of the same class. List 2 most important topics of the class. These topics will be used to generate summaries, they have to be consistent and accurate. The topic should be one to three words and should be a topic that is general to the course material. Do not prompt anything, simply list the 10 topics in a comma separated string.\"\n",
    "    messages_2 = [\n",
    "    {\"role\": \"system\", \"content\": system_message_2},\n",
    "    ]\n",
    "    x = questions.query.fetch_objects(\n",
    "        filters=Filter.by_property(\"filename\").equal(filename) & Filter.by_property(\"course\").equal(course),\n",
    "        return_properties=[\"content\"],\n",
    "    )\n",
    "    for i in range(len(x.objects)):\n",
    "        messages_2.append({\"role\": \"user\", \"content\": \"<content>\"+x.objects[i].properties['content']+\"<content>\"})\n",
    "\n",
    "    completion = client_open_ai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages_2\n",
    "    )\n",
    "\n",
    "    topics = completion.choices[0].message.content.split(\", \")\n",
    "    for i in topics:\n",
    "        topic_list.append(i)\n",
    "    print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauss elimination,Pivot\n",
      "Gauss elimination is a method used to transform a given matrix into row-echelon form, making it simpler to solve a system of linear equations. This process involves applying a sequence of elementary row operations to the matrix. The aim is to reach a row-echelon form, where the leading non-zero entries of each row are indented. Choosing a pivot wisely is essential in this process, as it helps in determining the non-zero entry row with the highest absolute value.\n",
      "\n",
      "Additionally, by applying Gauss elimination, a symmetric matrix can be factored into LU form. This method is especially effective for solving systems of equations. The process involves subtracting multiples of the first row from subsequent rows to create zero entries below the leading non-zero entries. Lower triangular inverses can also be found using this method in an inductive manner.\n",
      "\n",
      "Furthermore, the Gauss elimination process comprises three elementary row operations: swapping two rows, adding a multiple of one row to another, and scaling a row. In practice, these operations are carried out iteratively until the entire matrix is considered. The procedure results in a unique solution for the system. The matrix is transformed into upper echelon form, which offers a structured approach to solving linear equations. This method of Gauss elimination proves to be advantageous for various mathematical applications.\n",
      "Normal Approximation\n",
      "Normal Approximation to the Binomial Distribution enables the approximation of a binomial random variable with large parameters by a normal distribution. The standard normal distribution is often used to describe variables that cluster around the mean, for example, in cases like the heights of USA males. The Central Limit Theorem states that sums of independent random variables tend to be approximately Gaussian, making calculations more manageable. In machine learning, Gaussians are commonly used due to their simplification of calculations. Normalization is a necessary step to prevent nearest neighbors from being sensitive to different feature ranges. Completing the Square for Gaussians is a powerful technique to determine the distribution of Gaussian random variables. The Multivariate Gaussian Distribution can be defined for normally distributed variables following a Gaussian distribution. The 68-95-99.7 Rule helps in making quick approximate calculations. It can be illustrated by scenarios like calculating the probability of claims stemming from certain rare diseases. Lastly, the Poisson approximation to the Binomial distribution can be used to estimate the occurrence of rare events such as brain cancer cases in a given population.\n",
      "Probability\n",
      "In probability, events can be defined through various scenarios and calculations. For instance, the likelihood of specific outcomes in scenarios involving marbles, drawings, or exams can be determined mathematically. The probability of events like drawing marbles of different colors, correctly identifying items, or failing equipment during photo shoots can be calculated based on set parameters or conditions. Probability theory enables the handling of uncertain events by providing tools to analyze and predict outcomes in various situations. It is extensively applied across different fields due to its significance in addressing uncertainty and making informed decisions. A common approach is to associate probabilities with limiting frequencies or an individual's degree of belief, reflecting the likelihood of events occurring. Calculations involve formulas and methodologies to compute probabilities, such as binomial distributions, conditional probabilities, and sampling scenarios to understand outcomes like successful shots, meeting inspectors, or selecting specific answers in exams. Probabilistic analyses are instrumental in scenarios like survey sampling, where the chances of specific occurrences among a sample are computed to draw meaningful conclusions from data.\n",
      "Bias-Variance Decomposition\n",
      "Bias-Variance Decomposition is a critical concept in machine learning for understanding overfitting and underfitting. \n",
      "\n",
      "The bias-variance decomposition quantifies the trade-off between overly simple (underfitting) and overly complex (overfitting) hypothesis classes. A model with high bias is too simplistic to capture the structure in the data, while a model with low variance has enough data to provide a stable estimate.\n",
      "\n",
      "By considering the training set as random, we aim to understand how our learning algorithm is impacted by independently sampling training datasets. The process involves fixing a query point, sampling the true target from a conditional distribution, and repeating the procedure with random training datasets.\n",
      "\n",
      "This decomposition allows us to visualize predictions on a two-dimensional space, where each axis corresponds to predictions on two test examples. Additionally, the expected loss averages over points from the data distribution, introducing its type of variance to be considered.\n",
      "\n",
      "In a regression setup, the bias-variance decomposition analysis can be extended to calculate the expected loss by considering the expectation, variance, and other relevant parameters. Overall, this analysis is crucial for balancing model complexity and data representation effectively.\n",
      "Supervised Learning\n",
      "Supervised learning is a key component in machine learning where the learner predicts labels based on available data. This involves algorithms such as nearest neighbors, decision trees, ensembles, linear regression, logistic regression, and support vector machines (SVMs). In supervised learning, training sets with inputs and corresponding outputs are provided to make predictions, such as object categorization, image captioning, document classification, and speech-to-text conversion. The goal is to predict outcomes accurately based on the provided examples. Overly simple algorithms may underfit the data while overly complex ones can overfit it. Supervised learning aims to strike a balance between these extremes. Useful resources for understanding machine learning include books by Christopher Bishop, Kevin Murphy, David MacKay, and Shai Shalev-Shwartz & Shai Ben-David. Besides supervised learning, other paradigms include unsupervised and reinforcement learning. Linear classification methods are also important, including support vector machines and stochastic gradient descent for efficient processing of large datasets. Supervised learning is valuable as it allows algorithms to learn from data or experience, especially when programming the correct behavior manually is challenging.\n",
      "Conditional Expectation\n",
      "Conditional expectation is a fundamental concept that plays a crucial role in estimation and prediction tasks in statistics and probability theory. It refers to the expected value of a random variable given the information about another related random variable. Properties of conditional expectation include E [E ( X Y )] = E [X ], indicating the unbiasedness of the conditional expectation, and the optimality of E ( X Y ) as the best estimate of X when observing Y.\n",
      "\n",
      "In the context of continuous random variables, the expectation of a random variable X is defined as E (X) = ∫x f(x)dx, where f(x) is the probability density function of X. Additionally, in scenarios involving the prediction of a random variable Y based on X, the best deterministic value to predict Y given X is the conditional expectation E[Y|X]. Furthermore, the conditional expectation is related to the variance through E[Var(X|Y)] = Var[E(X|Y)].\n",
      "\n",
      "Moreover, in practical applications, like signal detection and estimation problems, understanding conditional expectation is essential. It allows for computing the expected value of received signals and finding conditional probability density functions in various scenarios. Overall, conditional expectation is a powerful concept that aids in making informed decisions and predictions based on available information.\n",
      "Joint Probability Distribution\n",
      "Joint probability distribution is a fundamental concept in probability theory that deals with the simultaneous behavior of multiple random variables. In the case where two random variables, X and Y, are jointly distributed, their joint probability density function (pdf) or joint probability mass function (pmf) can be determined. \n",
      "\n",
      "For discrete variables, the joint pmf is calculated as the probability that X takes on the value x and Y takes on the value y. The marginal pmfs of X and Y can be obtained by summing out the other variable from the joint pmf.\n",
      "\n",
      "When X and Y are continuous random variables, their joint probability density function is a non-negative function that characterizes their joint distribution for any given set. In this case, the marginal density functions of X and Y can be found by integrating the joint density function with respect to the other variable.\n",
      "\n",
      "The concept of conditional distributions plays a crucial role in understanding joint distributions. The conditional pmf or pdf of a random variable given another variable can be computed based on the joint distribution and the marginal distributions.\n",
      "\n",
      "Independence of random variables X and Y is established through the condition P(X = x and Y = y) = P(X = x) * P(Y = y). This condition implies that the behavior of one random variable does not affect the behavior of the other.\n",
      "Variance\n",
      "The variance of a random variable summarizes the spread or variability of the values it can take. It is defined as the expected value of the squared deviations from the mean. For a random variable X with mean µ, the variance is denoted by Var(X) = E[(X - µ)²].\n",
      "\n",
      "For discrete random variables, the variance is calculated as Var(X) = ∑[(xᵢ - µ)² * P(X = xᵢ)], where xᵢ are the possible values of X and P(X = xᵢ) is the probability of X taking the value xᵢ.\n",
      "\n",
      "For continuous random variables, the variance is computed as Var(X) = ∫[(x - µ)² * f(x)]dx, where f(x) is the probability density function of X.\n",
      "\n",
      "Understanding the variance is crucial as it provides insights into the level of dispersion in the data distribution. The variance of a random variable follows specific properties and can be decomposed in different ways to better understand the underlying data pattern.\n",
      "Expectation\n",
      "Expectation is a fundamental concept in probability theory that is used to estimate the long-term average of a random variable's outcomes. For a continuous random variable X with probability density function f(x), the expected value E(X) is defined as the integral of x*f(x) over the entire range of possible values for X. \n",
      "\n",
      "Properties of the expected value include linearity and the property that the expected value of the expected value of X given Y is equal to the expected value of X. \n",
      "\n",
      "In some scenarios, the expectation can be used to calculate various outcomes, such as in the case of workplace injuries following a Poisson distribution with a random parameter λ uniformly distributed on [0, 3]. The expectation of N, the number of workplace injuries, can be computed using the properties of expectation.\n",
      "\n",
      "Additionally, for a function g(X) of a discrete random variable X, the expectation of g(X) can be found by summing over all possible values of X weighted by the probability of each outcome. \n",
      "\n",
      "Furthermore, in games or decision-making situations, the concept of expected value can be used to determine the optimal choice based on potential outcomes and their associated probabilities. \n",
      "\n",
      "Overall, the expected value provides a measure of central tendency for random variables and is a key concept in probability theory with diverse applications in various fields like statistics, finance, and decision theory.\n",
      "Vector calculator\n",
      "A vector calculator is a tool that deals with arrays of numbers, where a vector is an array of numbers and a matrix is an array of vectors. It simplifies arithmetic operations and functions built into PostScript, such as addition, subtraction, multiplication, and division. It also supports vector and matrix operations, like vector addition, dot product, cross product, matrix multiplication, and transpose. In vector calculations, an origin point on a flat surface is used to determine vectors, allowing for scaling and addition of vectors. The choice of origin and operations like scalar multiplication make up the surface, with vectors having unique representations through their coordinates. Switching between vectorized and non-vectorized forms in programming can be beneficial for simpler and faster computations. Eigenvalues and eigenvectors can be found programmatically, with eigenvectors determined from the plane perpendicular to the vector. This method works efficiently for programmable calculators. The process involves finding eigenvectors and eigenvalues through matrix operations and calculations.\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-io0GiHcmNTwgY14uhJx68IeB on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(summary\u001b[38;5;241m.\u001b[39mobjects)):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(summary.objects[j].properties['content'])\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<content>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msummary\u001b[38;5;241m.\u001b[39mobjects[j]\u001b[38;5;241m.\u001b[39mproperties[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<content>\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m---> 21\u001b[0m completion \u001b[38;5;241m=\u001b[39m client_open_ai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m messages\u001b[38;5;241m=\u001b[39mmessages\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    669\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    670\u001b[0m             {\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    691\u001b[0m             },\n\u001b[1;32m    692\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    693\u001b[0m         ),\n\u001b[1;32m    694\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    695\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    696\u001b[0m         ),\n\u001b[1;32m    697\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    698\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    699\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    700\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    898\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    899\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    901\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    902\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    903\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    974\u001b[0m         options,\n\u001b[1;32m    975\u001b[0m         cast_to,\n\u001b[1;32m    976\u001b[0m         retries,\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    980\u001b[0m     )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1027\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    974\u001b[0m         options,\n\u001b[1;32m    975\u001b[0m         cast_to,\n\u001b[1;32m    976\u001b[0m         retries,\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    980\u001b[0m     )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1027\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    996\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-io0GiHcmNTwgY14uhJx68IeB on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "summary_list = []\n",
    "for i in topic_list:\n",
    "    \n",
    "    content_to_query = client.collections.get(\"lecture_content\")\n",
    "\n",
    "    summary = questions.query.near_text(\n",
    "        query=i,\n",
    "        limit=20\n",
    "    )\n",
    "\n",
    "    system_message = \"You will be provided with a list of course material (delimited with <content> XML tags) about the same topic. Generate an informative summary by taking in consideration all the content of the course material. The irrelevant and repetitive content can be filtered out. The summary should be concise and informative.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": \"The topic is \"+i}\n",
    "    ]\n",
    "\n",
    "    for j in range(len(summary.objects)):\n",
    "        # print(summary.objects[j].properties['content'])\n",
    "        messages.append({\"role\": \"user\", \"content\": \"<content>\"+summary.objects[j].properties['content']+\"<content>\"})\n",
    "\n",
    "    completion = client_open_ai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages\n",
    "    )\n",
    "\n",
    "    print(i)\n",
    "    print(completion.choices[0].message.content)\n",
    "    summary_list.append(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
