{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.4\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "weaviate_version = weaviate.__version__\n",
    "print(weaviate_version)\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "import weaviate\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os\n",
    "import weaviate.classes as wvc\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import DataSourceMetadata\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from weaviate.util import generate_uuid5\n",
    "import time\n",
    "import glob\n",
    "from uuid import uuid5, NAMESPACE_DNS\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /Users/ceciliaacosta/.cache/weaviate-embedded: process ID 15707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-04-30T13:28:12-04:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-04-30T13:28:12-04:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-04-30T13:28:12-04:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-04-30T13:28:13-04:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50050\",\"time\":\"2024-04-30T13:28:13-04:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-04-30T13:28:13-04:00\"}\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.connect_to_embedded(\n",
    "    version=\"latest\",  # e.g. version=\"1.23.10\"\n",
    "    headers={\n",
    "        \"X-HuggingFace-Api-Key\": \"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\" # Replace with your API key\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitiono PDF with unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_files(directory_path):\n",
    "    # Dictionary to hold file names and their elements\n",
    "    files_elements = {}\n",
    "\n",
    "    # Find all PDF files in the specified directory recursively\n",
    "    pdf_files = glob.glob(f\"{directory_path}/**/*.pdf\", recursive=True)\n",
    "\n",
    "    # Iterate over each PDF file found\n",
    "    for pdf_file in pdf_files:\n",
    "        # Extract elements from the PDF\n",
    "        elements = partition_pdf(pdf_file)\n",
    "\n",
    "        # Extract the folder name from the path of the PDF file\n",
    "        folder_name = os.path.basename(os.path.dirname(pdf_file))\n",
    "\n",
    "        # Convert each element to a string (assuming each element has a .text attribute or similar)\n",
    "        elements_text = [str(elem.text) if hasattr(elem, 'text') else str(elem) for elem in elements]\n",
    "\n",
    "        # Extract the file name without extension to use as a key\n",
    "        file_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "\n",
    "        # Create a unique key using folder name and file name\n",
    "        unique_key = (folder_name, file_name)\n",
    "\n",
    "        # Store the elements in the dictionary under the unique key\n",
    "        files_elements[unique_key] = elements_text\n",
    "\n",
    "        print(f\"Processed elements from {pdf_file} stored under key {unique_key}\")\n",
    "\n",
    "    return files_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For UT_CS411 NOT USED ANYMORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_questions_refined_UT(elements):\n",
    "#     text = \"\\n\".join(str(elem) for elem in elements)\n",
    "    \n",
    "#     # Patterns for main questions and subquestions\n",
    "#     main_question_pattern = re.compile(r'(\\d+)\\.\\s*(.*?)\\s*(?=\\n\\d+\\.|$)', re.DOTALL)\n",
    "#     subquestion_pattern = re.compile(r'\\n\\s*\\(([a-z])\\)\\s*(.*?)(?=\\n\\s*\\([a-z]\\)\\s*|\\n\\d+\\.|$)', re.DOTALL)\n",
    "    \n",
    "#     questions = {}\n",
    "    \n",
    "#     # Match and store main questions\n",
    "#     for main_match in main_question_pattern.finditer(text):\n",
    "#         main_question_number = main_match.group(1)\n",
    "#         main_question_text = main_match.group(2).strip()\n",
    "#         questions[main_question_number] = {\n",
    "#             \"text\": main_question_text.split(\"\\n\", 1)[0],\n",
    "#             \"subquestions\": []\n",
    "#         }\n",
    "\n",
    "#     # Match and store subquestions\n",
    "#     for sq_match in subquestion_pattern.finditer(text):\n",
    "#         subquestion_letter = sq_match.group(1).strip()\n",
    "#         subquestion_text = sq_match.group(2).strip()\n",
    "\n",
    "#         # Determine the appropriate main question for each subquestion\n",
    "#         previous_main_question_number = None\n",
    "#         for mq_number in questions:\n",
    "#             if text.find(\"\\n\" + mq_number + \".\") < sq_match.start():\n",
    "#                 previous_main_question_number = mq_number\n",
    "\n",
    "#         # Add the subquestion to the identified main question\n",
    "#         if previous_main_question_number:\n",
    "#             questions[previous_main_question_number][\"subquestions\"].append((subquestion_letter, subquestion_text))\n",
    "   \n",
    "#     return questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def parse_questions_combined_UBC_MT(elements):\n",
    "#     text = \"\\n\".join(str(elem) for elem in elements)\n",
    "    \n",
    "#     # Combined patterns for main questions and subquestions\n",
    "#     main_question_pattern = re.compile(r'(?:\\[\\d+\\]\\s*)?(\\d+)\\.\\s*(.*?)(?=(?:\\n(?:\\[\\d+\\]\\s*)?\\d+\\.|\\Z))', re.DOTALL)\n",
    "#     subquestion_pattern = re.compile(r'\\n\\s*\\(([a-z])\\)\\s*(.*?)(?=\\n\\s*\\([a-z]\\)\\s*|(?:\\n(?:\\[\\d+\\]\\s*)?\\d+\\.|\\Z))', re.DOTALL)\n",
    "    \n",
    "#     questions = {}\n",
    "    \n",
    "#     # Match and store main questions\n",
    "#     for main_match in main_question_pattern.finditer(text):\n",
    "#         main_question_number = main_match.group(1)\n",
    "#         main_question_text = main_match.group(2).strip()\n",
    "#         questions[main_question_number] = {\n",
    "#             \"text\": main_question_text.split(\"\\n\", 1)[0],\n",
    "#             \"subquestions\": [],\n",
    "#             \"end_position\": main_match.end()  # Store the end position of the main question\n",
    "#         }\n",
    "\n",
    "#     # Match and store subquestions\n",
    "#     for sq_match in subquestion_pattern.finditer(text):\n",
    "#         subquestion_letter = sq_match.group(1).strip()\n",
    "#         subquestion_text = sq_match.group(2).strip()\n",
    "\n",
    "#         # Determine the appropriate main question for each subquestion\n",
    "#         # by finding the main question whose end position is right before the subquestion\n",
    "#         previous_main_question_number = None\n",
    "#         previous_main_question_end_position = -1\n",
    "#         for mq_number, mq_details in questions.items():\n",
    "#             if mq_details[\"end_position\"] < sq_match.start() and mq_details[\"end_position\"] > previous_main_question_end_position:\n",
    "#                 previous_main_question_number = mq_number\n",
    "#                 previous_main_question_end_position = mq_details[\"end_position\"]\n",
    "\n",
    "#         # Add the subquestion to the identified main question\n",
    "#         if previous_main_question_number is not None:\n",
    "#             questions[previous_main_question_number][\"subquestions\"].append((subquestion_letter, subquestion_text))\n",
    "   \n",
    "#     return questions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global pattern matching for ALL exams for ALL courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_exam_questions(text):\n",
    "    text = \"\\n\".join(str(elem) for elem in text)\n",
    "    questions = {}\n",
    "    # # Subquestion regex matches a letter in parentheses, followed by any text, and ensures it's not immediately followed by another subquestion or main question.\n",
    "    subquestion_pattern = re.compile(r'\\n\\s*\\(([a-z])\\)\\s*(.*?)(?=\\n\\s*\\([a-z]\\)\\s*|\\n\\d+\\.|\\Z)', re.DOTALL)\n",
    "    # Main question regex to match \"Question\" or a number followed by a period at the start of a line, potentially with bracketed points\n",
    "    main_question_pattern = re.compile(r'^(?:\\[\\d+\\]\\s*)?(?:Question\\s+|Problem\\s+)?(\\d+)\\.\\s*(.*?)(?=\\n(?:\\[\\d+\\]\\s*)?(?:Question\\s+|Problem\\s+)?\\d+\\.|\\Z)', re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    alternative_subquestion_pattern = re.compile(r'\\n\\s*([a-z])\\)\\s*(.*?)\\s*(?=\\n\\s*[a-z]\\)\\s*|\\n\\d+\\.|\\Z)', re.DOTALL)\n",
    "    \n",
    "    # Match and store main questions\n",
    "    for main_match in main_question_pattern.finditer(text):\n",
    "        #print(main_match , \"-----------------------------------------------MAINMATCH\")\n",
    "        main_question_number = main_match.group(1)\n",
    "        main_question_text = main_match.group(2).strip()\n",
    "        \n",
    "\n",
    "        # check if the main question number is already in the dictionary\n",
    "        if main_question_number in questions:\n",
    "            # If the text is empty, replace it with the new text\n",
    "            if not questions[main_question_number][\"text\"]:\n",
    "                questions[main_question_number][\"text\"] = main_question_text\n",
    "            else:\n",
    "                # Skip this main question if it already has text\n",
    "                continue\n",
    "            \n",
    "        # Initialize a new main question with potential placeholder for text\n",
    "        questions[main_question_number] = {\n",
    "            \"text\": \"\",\n",
    "            \"subquestions\": []\n",
    "        }\n",
    "\n",
    "        # If there is text for the main question, set it\n",
    "        if main_question_text:\n",
    "            questions[main_question_number][\"text\"] = main_question_text\n",
    "\n",
    "    # If no main questions are found, look for exceptions\n",
    "    if not questions:\n",
    "\n",
    "         # Match and store main questions\n",
    "        main_question_pattern = re.compile(r'Problem\\s+(\\d+):\\s*(.*?)(?=(?:\\n\\[?\\d+\\])?\\s*Problem\\s+\\d+:|\\Z)', re.DOTALL)\n",
    "        \n",
    "        for main_match in main_question_pattern.finditer(text):\n",
    "            #print(main_match , \"-----------------------------------------------MAINMATCH\")\n",
    "            question_num = main_match.group(1)\n",
    "            question_text = main_match.group(2).strip()\n",
    "            questions[question_num] = {\"text\": question_text, \"subquestions\": []}\n",
    "\n",
    "        # Match and store subquestions\n",
    "        subquestion_pattern = re.compile(r'\\[(\\d+)\\]\\s*\\(([a-z])\\)\\s*(.*?)(?=\\n\\[\\d+\\]\\s*\\([a-z]\\)|\\nProblem\\s+\\d+:|\\Z)', re.DOTALL)\n",
    "        \n",
    "        for sq_match in subquestion_pattern.finditer(text):\n",
    "            subquestion_letter = sq_match.group(2)\n",
    "            subquestion_text = sq_match.group(3).strip()\n",
    "\n",
    "            # Add the subquestion to the closest preceding main question\n",
    "            for q_num in reversed(questions.keys()):\n",
    "                if text.rfind('Problem ' + q_num + ':', 0, sq_match.start()) != -1:\n",
    "                    questions[q_num][\"subquestions\"].append((subquestion_letter, subquestion_text))\n",
    "                    break\n",
    "\n",
    "\n",
    "    # Match and store subquestions\n",
    "    for sq_match in subquestion_pattern.finditer(text):\n",
    "\n",
    "        subquestion_letter = sq_match.group(1).strip()\n",
    "        subquestion_text = sq_match.group(2).strip()\n",
    "        \n",
    "        # Find the closest preceding main question number\n",
    "        closest_main_question_number = max(filter(lambda num: text.find(\"\\n\" + num + \".\") < sq_match.start(), questions), default=None)\n",
    "        \n",
    "        if closest_main_question_number:\n",
    "            # Add subquestion text to the main question if it's empty\n",
    "            if not questions[closest_main_question_number][\"text\"]:\n",
    "                questions[closest_main_question_number][\"text\"] = subquestion_text\n",
    "            else:\n",
    "                questions[closest_main_question_number][\"subquestions\"].append((subquestion_letter, subquestion_text))\n",
    "\n",
    "    # If no subquestions with parentheses are found, try without parentheses\n",
    "    if all(len(q[\"subquestions\"]) == 0 for q in questions.values()):\n",
    "        print(\"No subquestions found with parentheses, trying without...\")  \n",
    "        for sq_match in alternative_subquestion_pattern.finditer(text):\n",
    "            print(\"alternative_subquestion_pattern\")\n",
    "            subquestion_letter = sq_match.group(1)\n",
    "            subquestion_text = sq_match.group(2).strip()\n",
    "                    # Find the closest preceding main question number\n",
    "            closest_main_question_number = max(filter(lambda num: text.find(\"\\n\" + num + \".\") < sq_match.start(), questions), default=None)\n",
    "            \n",
    "            if closest_main_question_number:\n",
    "                # Add subquestion text to the main question if it's empty\n",
    "                if not questions[closest_main_question_number][\"text\"]:\n",
    "                    questions[closest_main_question_number][\"text\"] = subquestion_text\n",
    "                else:\n",
    "                    questions[closest_main_question_number][\"subquestions\"].append((subquestion_letter, subquestion_text))\n",
    "\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Use actual OCR-extracted text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process file elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 89.06%, threshold set to 80.00%\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate\",\"time\":\"2024-04-30T13:28:13-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/objects/segment-1714407085741440000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/objects/segment-1714405707628248000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Test\",\"index\":\"test\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/test/JWRqUX6eGimL/lsm/objects/segment-1714405707628344000\",\"shard\":\"JWRqUX6eGimL\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_subQuestion/segment-1714407085743849000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property__id/segment-1714407085743783000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_subQuestion_searchable/segment-1714407085744550000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_course/segment-1714407085744012000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Test\",\"index\":\"test\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/test/JWRqUX6eGimL/lsm/property__id/segment-1714405707633676000\",\"shard\":\"JWRqUX6eGimL\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Test\",\"index\":\"test\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/test/JWRqUX6eGimL/lsm/property_content/segment-1714405707634422000\",\"shard\":\"JWRqUX6eGimL\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_filename/segment-1714407085743727000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Test\",\"index\":\"test\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/test/JWRqUX6eGimL/lsm/property_filename/segment-1714405707634941000\",\"shard\":\"JWRqUX6eGimL\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_course_searchable/segment-1714407085744803000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/property__id/segment-1714405707635050000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_mainQuestion/segment-1714407085743905000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_filename_searchable/segment-1714407085744284000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/property_course/segment-1714405707635296000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/property_filename/segment-1714405707636610000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_questionNumber/segment-1714407085743980000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_mainQuestion_searchable/segment-1714407085744478000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Exam_content\",\"index\":\"exam_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/exam_content/PhTvGR1DAP4M/lsm/property_questionNumber_searchable/segment-1714407085744702000\",\"shard\":\"PhTvGR1DAP4M\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/property_content/segment-1714405707637137000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Test\",\"index\":\"test\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/test/JWRqUX6eGimL/lsm/property_content_searchable/segment-1714405707638799000\",\"shard\":\"JWRqUX6eGimL\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/property_course_searchable/segment-1714405707638693000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/property_filename_searchable/segment-1714405707639427000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Test\",\"index\":\"test\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/test/JWRqUX6eGimL/lsm/property_filename_searchable/segment-1714405707638688000\",\"shard\":\"JWRqUX6eGimL\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"lsm_recover_from_active_wal\",\"class\":\"Lecture_content\",\"index\":\"lecture_content\",\"level\":\"warning\",\"msg\":\"active write-ahead-log found. Did weaviate crash prior to this? Trying to recover...\",\"path\":\"/Users/ceciliaacosta/.local/share/weaviate/lecture_content/KWBcS9RdvmOW/lsm/property_content_searchable/segment-1714405707641060000\",\"shard\":\"KWBcS9RdvmOW\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard test_JWRqUX6eGimL in 32.064342ms\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard exam_content_PhTvGR1DAP4M in 32.067888ms\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-04-30T13:28:14-04:00\",\"took\":679061}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-04-30T13:28:14-04:00\",\"took\":920429}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard lecture_content_KWBcS9RdvmOW in 38.654862ms\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":5000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-04-30T13:28:14-04:00\",\"took\":174669128}\n",
      "{\"action\":\"telemetry_push\",\"level\":\"info\",\"msg\":\"telemetry started\",\"payload\":\"\\u0026{MachineID:94880d1c-0126-4555-b451-a23259f1081e Type:INIT Version:1.24.6 Modules:generative-openai,qna-openai,ref2vec-centroid,reranker-cohere,text2vec-cohere,text2vec-huggingface,text2vec-openai NumObjects:0 OS:darwin Arch:amd64}\",\"time\":\"2024-04-30T13:28:14-04:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed elements from data/test/UBC_MT307/307_2015WT1.pdf stored under key ('UBC_MT307', '307_2015WT1')\n",
      "Processed elements from data/test/UBC_MT307/307_2012WT2.pdf stored under key ('UBC_MT307', '307_2012WT2')\n",
      "Processed elements from data/test/UBC_MT307/307_2014WT1.pdf stored under key ('UBC_MT307', '307_2014WT1')\n",
      "Processed elements from data/test/UT_CS411/CSC311f19_final.pdf stored under key ('UT_CS411', 'CSC311f19_final')\n",
      "Processed elements from data/test/UT_CS411/CSC411f18_midterm2.pdf stored under key ('UT_CS411', 'CSC411f18_midterm2')\n",
      "Processed elements from data/test/UT_CS411/CSC411f18_midterm1.pdf stored under key ('UT_CS411', 'CSC411f18_midterm1')\n",
      "Processed elements from data/test/UBC_ST302/midterm201.pdf stored under key ('UBC_ST302', 'midterm201')\n",
      "Processed elements from data/test/UBC_ST302/midterm202.pdf stored under key ('UBC_ST302', 'midterm202')\n",
      "Processed elements from data/test/UBC_ST302/1999-final.pdf stored under key ('UBC_ST302', '1999-final')\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the PDF files\n",
    "directory_path = 'data/test'\n",
    "directory= os.path.split(directory_path)[-1]\n",
    "print(directory)\n",
    "#process the pdf files\n",
    "files_elements = process_pdf_files(directory_path)\n",
    "#print(files_elements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not used it was for each individual course only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name, elements in files_elements.items():\n",
    "#     parsed_questions ={}\n",
    "\n",
    "#     # Apply the parse_questions function to the elements of the current file\n",
    "#     if file_name.startswith(\"307\"):\n",
    "#         print(file_name)\n",
    "#         parsed_questions = parse_questions_combined_UBC_MT(files_elements[file_name])\n",
    "#     else:\n",
    "#         print(file_name)\n",
    "#         parsed_questions = parse_questions_refined_UT(files_elements[file_name])\n",
    "\n",
    "#     # Iterate through the parsed questions and print them along with their subquestions\n",
    "#     for question_num, question_info in parsed_questions.items():\n",
    "#         #print(f\"Processing question_num: {question_num}, question_info: {question_info}\")\n",
    "#         print(f\"Question {question_num}: {question_info['text']}\")\n",
    "#         for subq in question_info[\"subquestions\"]:\n",
    "#             print(f\" ----sub {subq[0]}: {subq[1]}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name, elements in files_elements.items():\n",
    "#     parsed_questions ={}\n",
    "\n",
    "#     # Apply the parse_questions function to the elements of the current file\n",
    "#     print(file_name)\n",
    "#     parsed_questions = parse_exam_questions(files_elements[file_name])\n",
    "#     if not parsed_questions:  # Add a test to see if the content is empty\n",
    "#         print(f\"------------------------------------------------No content found for exam {file_name}.\\n\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "#     # Iterate through the parsed questions and print them along with their subquestions\n",
    "#     for question_num, question_info in parsed_questions.items():\n",
    "#         #print(f\"Processing question_num: {question_num}, question_info: {question_info}\")\n",
    "#         print(f\"QUESTION------------------ {question_num}: {question_info['text']}\")\n",
    "#         for subq in question_info[\"subquestions\"]:\n",
    "#             print(f\" -------sub {subq[0]}: {subq[1]}\")\n",
    "#     print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard exam_content_GmSrCya1RVF4 in 7.579943ms\",\"time\":\"2024-04-30T13:28:17-04:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-04-30T13:28:17-04:00\",\"took\":128747}\n"
     ]
    }
   ],
   "source": [
    "client.collections.delete(name=\"exam_content\")\n",
    "\n",
    "\n",
    "questions = client.collections.create(\n",
    "\"exam_content\",\n",
    "vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_huggingface(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        vectorize_collection_name=True\n",
    "),\n",
    "properties=[  \n",
    "    wvc.config.Property(name=\"mainQuestion\", data_type=wvc.config.DataType.TEXT),\n",
    "    wvc.config.Property(name=\"QuestionNumber\", data_type=wvc.config.DataType.TEXT),\n",
    "    wvc.config.Property(name=\"subQuestion\", data_type=wvc.config.DataType.TEXT),\n",
    "    wvc.config.Property(name=\"filename\", data_type=wvc.config.DataType.TEXT),\n",
    "    # Add a 'course' property to store the course directory name\n",
    "    wvc.config.Property(name=\"course\", data_type=wvc.config.DataType.TEXT),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the objects that will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No subquestions found with parentheses, trying without...\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "alternative_subquestion_pattern\n",
      "No subquestions found with parentheses, trying without...\n"
     ]
    }
   ],
   "source": [
    "question_objs = list()\n",
    "count = 0\n",
    "for file_course, elements in files_elements.items():\n",
    "\n",
    "    parsed_questions ={}\n",
    "\n",
    "    # # Apply the parse_questions function to the elements of the current file\n",
    "    # if file_course[1].startswith(\"307\"):\n",
    "    #      print(file_course)\n",
    "    #      parsed_questions = parse_questions_combined_UBC_MT(files_elements[file_course])\n",
    "    # # else:\n",
    "    #     print(file_name)\n",
    "    #     parsed_questions = parse_questions_refined_UT(files_elements[file_name])\n",
    "   \n",
    "\n",
    "    #print(file_course[0])\n",
    "    #print(file_course[1])\n",
    "    parsed_questions= parse_exam_questions(files_elements[file_course])\n",
    "\n",
    "    \n",
    "    for question, details in parsed_questions.items():  # Batch import data\n",
    "        #print(f\"-----------------FILENAME: {file_course[1]}\") # Print the filename\n",
    "        #print(f\"---------importing question: {question}, details: {details}\")\n",
    "        if len(details[\"subquestions\"]) == 0:  # if there are no subquestions\n",
    "            properties = {\n",
    "                \"mainQuestion\": details[\"text\"],\n",
    "                \"QuestionNumber\": question,\n",
    "                \"subQuestion\": \"\",\n",
    "                \"filename\": file_course[1],\n",
    "                \"course\": file_course[0]\n",
    "            }\n",
    "            question_objs.append(properties)\n",
    "        else: \n",
    "            for subq in details[\"subquestions\"]:\n",
    "                #print(f\"-----importing subquestion: {subq[1]}\")\n",
    "            \n",
    "                properties = {\n",
    "                    \"mainQuestion\": details[\"text\"],\n",
    "                    \"QuestionNumber\": question,\n",
    "                    \"subQuestion\": subq[1],\n",
    "                    \"filename\": file_course[1],\n",
    "                    \"course\": file_course[0]\n",
    "                }\n",
    "                question_objs.append(properties)\n",
    "        count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 60 questions\n"
     ]
    }
   ],
   "source": [
    "print(f\"Importing {count} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 60\n"
     ]
    }
   ],
   "source": [
    "# ===== Batch import =====\n",
    "with questions.batch.dynamic() as batch:\n",
    "    for data_row in question_objs:\n",
    "        obj_uuid = uuid5(NAMESPACE_DNS, data_row[\"mainQuestion\"]+ data_row[\"filename\"])\n",
    "        batch.add_object(\n",
    "            properties=data_row,\n",
    "            uuid=obj_uuid\n",
    "        )\n",
    "        \n",
    "print(\"Total chunks:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks in Wv: 60\n"
     ]
    }
   ],
   "source": [
    "lecture_content = client.collections.get(\"exam_content\")\n",
    "response_ttl = lecture_content.aggregate.over_all(total_count=True)\n",
    "\n",
    "print( \"Total Chunks in Wv:\",response_ttl.total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift-6758",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
